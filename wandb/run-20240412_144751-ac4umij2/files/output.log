
  1%|▉                                                                                  | 1/84 [00:01<01:38,  1.18s/it]
stem.conv.weight
stem.conv.bias
stem.norm.weight
stem.norm.bias
stages.0.blocks.0.0.cpe1.proj.weight
stages.0.blocks.0.0.cpe1.proj.bias
stages.0.blocks.0.0.norm1.weight
stages.0.blocks.0.0.norm1.bias
stages.0.blocks.0.0.attn.qkv.weight
stages.0.blocks.0.0.attn.qkv.bias
stages.0.blocks.0.0.attn.proj.weight
stages.0.blocks.0.0.attn.proj.bias
stages.0.blocks.0.0.cpe2.proj.weight
stages.0.blocks.0.0.cpe2.proj.bias
stages.0.blocks.0.0.norm2.weight
stages.0.blocks.0.0.norm2.bias
stages.0.blocks.0.0.mlp.fc1.weight
stages.0.blocks.0.0.mlp.fc1.bias
stages.0.blocks.0.0.mlp.fc2.weight
stages.0.blocks.0.0.mlp.fc2.bias
stages.0.blocks.0.1.cpe1.proj.weight
stages.0.blocks.0.1.cpe1.proj.bias
stages.0.blocks.0.1.norm1.weight
stages.0.blocks.0.1.norm1.bias
stages.0.blocks.0.1.attn.qkv.weight
stages.0.blocks.0.1.attn.qkv.bias
stages.0.blocks.0.1.attn.proj.weight
stages.0.blocks.0.1.attn.proj.bias
stages.0.blocks.0.1.cpe2.proj.weight
stages.0.blocks.0.1.cpe2.proj.bias
stages.0.blocks.0.1.norm2.weight
stages.0.blocks.0.1.norm2.bias
stages.0.blocks.0.1.mlp.fc1.weight
stages.0.blocks.0.1.mlp.fc1.bias
stages.0.blocks.0.1.mlp.fc2.weight
stages.0.blocks.0.1.mlp.fc2.bias
stages.1.downsample.norm.weight
stages.1.downsample.norm.bias
stages.1.downsample.conv.weight
stages.1.downsample.conv.bias
stages.1.blocks.0.0.cpe1.proj.weight
stages.1.blocks.0.0.cpe1.proj.bias
stages.1.blocks.0.0.norm1.weight
stages.1.blocks.0.0.norm1.bias
stages.1.blocks.0.0.attn.qkv.weight
stages.1.blocks.0.0.attn.qkv.bias
stages.1.blocks.0.0.attn.proj.weight
stages.1.blocks.0.0.attn.proj.bias
stages.1.blocks.0.0.cpe2.proj.weight
stages.1.blocks.0.0.cpe2.proj.bias
stages.1.blocks.0.0.norm2.weight
stages.1.blocks.0.0.norm2.bias
stages.1.blocks.0.0.mlp.fc1.weight
stages.1.blocks.0.0.mlp.fc1.bias
stages.1.blocks.0.0.mlp.fc2.weight
stages.1.blocks.0.0.mlp.fc2.bias
stages.1.blocks.0.1.cpe1.proj.weight
stages.1.blocks.0.1.cpe1.proj.bias
stages.1.blocks.0.1.norm1.weight
stages.1.blocks.0.1.norm1.bias
stages.1.blocks.0.1.attn.qkv.weight
stages.1.blocks.0.1.attn.qkv.bias
stages.1.blocks.0.1.attn.proj.weight
stages.1.blocks.0.1.attn.proj.bias
stages.1.blocks.0.1.cpe2.proj.weight
stages.1.blocks.0.1.cpe2.proj.bias
stages.1.blocks.0.1.norm2.weight
stages.1.blocks.0.1.norm2.bias
stages.1.blocks.0.1.mlp.fc1.weight
stages.1.blocks.0.1.mlp.fc1.bias
stages.1.blocks.0.1.mlp.fc2.weight
stages.1.blocks.0.1.mlp.fc2.bias
stages.2.downsample.norm.weight
stages.2.downsample.norm.bias
stages.2.downsample.conv.weight
stages.2.downsample.conv.bias
stages.2.blocks.0.0.cpe1.proj.weight
stages.2.blocks.0.0.cpe1.proj.bias
stages.2.blocks.0.0.norm1.weight
stages.2.blocks.0.0.norm1.bias
stages.2.blocks.0.0.attn.qkv.weight
stages.2.blocks.0.0.attn.qkv.bias
stages.2.blocks.0.0.attn.proj.weight
stages.2.blocks.0.0.attn.proj.bias
stages.2.blocks.0.0.cpe2.proj.weight
stages.2.blocks.0.0.cpe2.proj.bias
stages.2.blocks.0.0.norm2.weight
stages.2.blocks.0.0.norm2.bias
stages.2.blocks.0.0.mlp.fc1.weight
stages.2.blocks.0.0.mlp.fc1.bias
stages.2.blocks.0.0.mlp.fc2.weight
stages.2.blocks.0.0.mlp.fc2.bias
stages.2.blocks.0.1.cpe1.proj.weight
stages.2.blocks.0.1.cpe1.proj.bias
stages.2.blocks.0.1.norm1.weight
stages.2.blocks.0.1.norm1.bias
stages.2.blocks.0.1.attn.qkv.weight
stages.2.blocks.0.1.attn.qkv.bias
stages.2.blocks.0.1.attn.proj.weight
stages.2.blocks.0.1.attn.proj.bias
stages.2.blocks.0.1.cpe2.proj.weight
stages.2.blocks.0.1.cpe2.proj.bias
stages.2.blocks.0.1.norm2.weight
stages.2.blocks.0.1.norm2.bias
stages.2.blocks.0.1.mlp.fc1.weight
stages.2.blocks.0.1.mlp.fc1.bias
stages.2.blocks.0.1.mlp.fc2.weight
stages.2.blocks.0.1.mlp.fc2.bias
stages.2.blocks.1.0.cpe1.proj.weight
stages.2.blocks.1.0.cpe1.proj.bias
stages.2.blocks.1.0.norm1.weight
stages.2.blocks.1.0.norm1.bias
stages.2.blocks.1.0.attn.qkv.weight
stages.2.blocks.1.0.attn.qkv.bias
stages.2.blocks.1.0.attn.proj.weight
stages.2.blocks.1.0.attn.proj.bias
stages.2.blocks.1.0.cpe2.proj.weight
stages.2.blocks.1.0.cpe2.proj.bias
stages.2.blocks.1.0.norm2.weight
stages.2.blocks.1.0.norm2.bias
stages.2.blocks.1.0.mlp.fc1.weight
stages.2.blocks.1.0.mlp.fc1.bias
stages.2.blocks.1.0.mlp.fc2.weight
stages.2.blocks.1.0.mlp.fc2.bias
stages.2.blocks.1.1.cpe1.proj.weight
stages.2.blocks.1.1.cpe1.proj.bias
stages.2.blocks.1.1.norm1.weight
stages.2.blocks.1.1.norm1.bias
stages.2.blocks.1.1.attn.qkv.weight
stages.2.blocks.1.1.attn.qkv.bias
stages.2.blocks.1.1.attn.proj.weight
stages.2.blocks.1.1.attn.proj.bias
stages.2.blocks.1.1.cpe2.proj.weight
stages.2.blocks.1.1.cpe2.proj.bias
stages.2.blocks.1.1.norm2.weight
stages.2.blocks.1.1.norm2.bias
stages.2.blocks.1.1.mlp.fc1.weight
stages.2.blocks.1.1.mlp.fc1.bias
stages.2.blocks.1.1.mlp.fc2.weight
stages.2.blocks.1.1.mlp.fc2.bias
stages.2.blocks.2.0.cpe1.proj.weight
stages.2.blocks.2.0.cpe1.proj.bias
stages.2.blocks.2.0.norm1.weight
stages.2.blocks.2.0.norm1.bias
stages.2.blocks.2.0.attn.qkv.weight
stages.2.blocks.2.0.attn.qkv.bias
stages.2.blocks.2.0.attn.proj.weight
stages.2.blocks.2.0.attn.proj.bias
stages.2.blocks.2.0.cpe2.proj.weight
stages.2.blocks.2.0.cpe2.proj.bias
stages.2.blocks.2.0.norm2.weight
stages.2.blocks.2.0.norm2.bias
stages.2.blocks.2.0.mlp.fc1.weight
stages.2.blocks.2.0.mlp.fc1.bias
stages.2.blocks.2.0.mlp.fc2.weight
stages.2.blocks.2.0.mlp.fc2.bias
stages.2.blocks.2.1.cpe1.proj.weight
stages.2.blocks.2.1.cpe1.proj.bias
stages.2.blocks.2.1.norm1.weight
stages.2.blocks.2.1.norm1.bias
stages.2.blocks.2.1.attn.qkv.weight
stages.2.blocks.2.1.attn.qkv.bias
stages.2.blocks.2.1.attn.proj.weight
stages.2.blocks.2.1.attn.proj.bias
stages.2.blocks.2.1.cpe2.proj.weight
stages.2.blocks.2.1.cpe2.proj.bias
stages.2.blocks.2.1.norm2.weight
stages.2.blocks.2.1.norm2.bias
stages.2.blocks.2.1.mlp.fc1.weight
stages.2.blocks.2.1.mlp.fc1.bias
stages.2.blocks.2.1.mlp.fc2.weight
stages.2.blocks.2.1.mlp.fc2.bias
stages.3.downsample.norm.weight
stages.3.downsample.norm.bias
stages.3.downsample.conv.weight
stages.3.downsample.conv.bias
stages.3.blocks.0.0.cpe1.proj.weight
stages.3.blocks.0.0.cpe1.proj.bias
stages.3.blocks.0.0.norm1.weight
stages.3.blocks.0.0.norm1.bias
stages.3.blocks.0.0.attn.qkv.weight
stages.3.blocks.0.0.attn.qkv.bias
stages.3.blocks.0.0.attn.proj.weight
stages.3.blocks.0.0.attn.proj.bias
stages.3.blocks.0.0.cpe2.proj.weight
stages.3.blocks.0.0.cpe2.proj.bias
stages.3.blocks.0.0.norm2.weight
stages.3.blocks.0.0.norm2.bias
stages.3.blocks.0.0.mlp.fc1.weight
stages.3.blocks.0.0.mlp.fc1.bias
stages.3.blocks.0.0.mlp.fc2.weight
stages.3.blocks.0.0.mlp.fc2.bias
stages.3.blocks.0.1.cpe1.proj.weight
stages.3.blocks.0.1.cpe1.proj.bias
stages.3.blocks.0.1.norm1.weight
stages.3.blocks.0.1.norm1.bias
stages.3.blocks.0.1.attn.qkv.weight
stages.3.blocks.0.1.attn.qkv.bias
stages.3.blocks.0.1.attn.proj.weight
stages.3.blocks.0.1.attn.proj.bias
stages.3.blocks.0.1.cpe2.proj.weight
stages.3.blocks.0.1.cpe2.proj.bias
stages.3.blocks.0.1.norm2.weight
stages.3.blocks.0.1.norm2.bias
stages.3.blocks.0.1.mlp.fc1.weight
stages.3.blocks.0.1.mlp.fc1.bias
stages.3.blocks.0.1.mlp.fc2.weight
stages.3.blocks.0.1.mlp.fc2.bias
head.norm.weight
head.norm.bias
head.fc.weight
head.fc.bias
head.fc.bias
DaVit(
  (stem): Stem(
    (conv): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
    (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)
  )
  (stages): Sequential(
    (0): DaVitStage(
      (downsample): Identity()
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (1): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (2): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
        (1): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
        (2): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (3): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
  )
  (norm_pre): Identity()
  (head): NormMlpClassifierHead(
    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
    (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (pre_logits): Identity()
    (drop): Dropout(p=0.0, inplace=False)
    (fc): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=768, out_features=64, bias=False)
      (2): GELU(approximate='none')
      (3): Linear(in_features=64, out_features=1, bias=False)
      (4): Sigmoid()
    )
  )
)
Starting training...
--------------------




































 96%|███████████████████████████████████████████████████████████████████████████████   | 81/84 [01:13<00:02,  1.11it/s]
New threshold is 0.4225771725177765
train F1 is 0.4546360969543457
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:15<00:00,  1.11it/s]




 95%|██████████████████████████████████████████████████████████████████████████████    | 20/21 [00:08<00:00,  2.45it/s]
New threshold is 0.43327367305755615
val F1 is 0.5247148275375366
Epoch 1/40, learning rate: 1.9864292578550327e-05
Train Loss: 0.6778, Train Acc: 0.5309, Train f1: 0.4546, Train Precision: 0.4368, Train Recall: 0.4740, Train AUC: 0.5220
Valitadion Loss: 0.6746, Validation Acc: 0.5629, Vall f1: 0.5247, Val Precision: 0.4792, Val Recall: 0.5798, Val AUC: 0.5902
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.51it/s]




































 96%|███████████████████████████████████████████████████████████████████████████████   | 81/84 [01:13<00:02,  1.11it/s]
New threshold is 0.41922876238822937
train F1 is 0.535315990447998
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:15<00:00,  1.12it/s]



 76%|██████████████████████████████████████████████████████████████▍                   | 16/21 [00:06<00:02,  2.43it/s]
New threshold is 0.3457188606262207
val F1 is 0.5977859497070312
Epoch 2/40, learning rate: 1.9460853615232776e-05
Train Loss: 0.6670, Train Acc: 0.5712, Train f1: 0.5353, Train Precision: 0.4840, Train Recall: 0.5988, Train AUC: 0.5826
Valitadion Loss: 0.6787, Validation Acc: 0.6189, Vall f1: 0.5978, Val Precision: 0.5329, Val Recall: 0.6807, Val AUC: 0.6542
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.49it/s]





































 98%|████████████████████████████████████████████████████████████████████████████████  | 82/84 [01:14<00:01,  1.12it/s]
New threshold is 0.3975944221019745
train F1 is 0.5315789580345154
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:15<00:00,  1.11it/s]



 76%|██████████████████████████████████████████████████████████████▍                   | 16/21 [00:06<00:02,  2.43it/s]
New threshold is 0.481259822845459
val F1 is 0.5555555820465088
Epoch 3/40, learning rate: 1.8800633042879672e-05
Train Loss: 0.6728, Train Acc: 0.5420, Train f1: 0.5316, Train Precision: 0.4598, Train Recall: 0.6299, Train AUC: 0.5589
Valitadion Loss: 0.6847, Validation Acc: 0.5804, Vall f1: 0.5556, Val Precision: 0.4967, Val Recall: 0.6303, Val AUC: 0.5760
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.49it/s]





































 98%|████████████████████████████████████████████████████████████████████████████████  | 82/84 [01:14<00:01,  1.12it/s]
New threshold is 0.43188920617103577
train F1 is 0.5248780250549316
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:15<00:00,  1.12it/s]



 81%|██████████████████████████████████████████████████████████████████▍               | 17/21 [00:07<00:01,  2.43it/s]
New threshold is 0.4832375645637512
val F1 is 0.589090883731842
Epoch 4/40, learning rate: 1.79015502286794e-05
Train Loss: 0.6609, Train Acc: 0.5823, Train f1: 0.5249, Train Precision: 0.4945, Train Recall: 0.5593, Train AUC: 0.6092
Valitadion Loss: 0.6687, Validation Acc: 0.6049, Vall f1: 0.5891, Val Precision: 0.5192, Val Recall: 0.6807, Val AUC: 0.6411
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.48it/s]





































 98%|████████████████████████████████████████████████████████████████████████████████  | 82/84 [01:14<00:01,  1.12it/s]
New threshold is 0.4437443017959595
train F1 is 0.5339999794960022
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:15<00:00,  1.11it/s]



 86%|██████████████████████████████████████████████████████████████████████▎           | 18/21 [00:07<00:01,  2.44it/s]
New threshold is 0.49567824602127075
val F1 is 0.5714285969734192
Epoch 5/40, learning rate: 1.678800761592905e-05
Train Loss: 0.6582, Train Acc: 0.6003, Train f1: 0.5340, Train Precision: 0.5145, Train Recall: 0.5551, Train AUC: 0.6155
Valitadion Loss: 0.6713, Validation Acc: 0.6119, Vall f1: 0.5714, Val Precision: 0.5286, Val Recall: 0.6218, Val AUC: 0.6406
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.49it/s]





































 98%|████████████████████████████████████████████████████████████████████████████████  | 82/84 [01:14<00:01,  1.11it/s]
New threshold is 0.4246106445789337
train F1 is 0.5845864415168762
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:15<00:00,  1.11it/s]



 86%|██████████████████████████████████████████████████████████████████████▎           | 18/21 [00:07<00:01,  2.42it/s]
New threshold is 0.4708743095397949
val F1 is 0.5746268630027771
Epoch 6/40, learning rate: 1.549022840546991e-05
Train Loss: 0.6410, Train Acc: 0.6209, Train f1: 0.5846, Train Precision: 0.5334, Train Recall: 0.6466, Train AUC: 0.6607
Valitadion Loss: 0.6577, Validation Acc: 0.6014, Vall f1: 0.5746, Val Precision: 0.5168, Val Recall: 0.6471, Val AUC: 0.6329
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.48it/s]





































 99%|█████████████████████████████████████████████████████████████████████████████████ | 83/84 [01:15<00:00,  1.11it/s]
New threshold is 0.4425828754901886
train F1 is 0.6090373396873474
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:15<00:00,  1.11it/s]



 86%|██████████████████████████████████████████████████████████████████████▎           | 18/21 [00:07<00:01,  2.40it/s]
New threshold is 0.39499613642692566
val F1 is 0.6136363744735718
Epoch 7/40, learning rate: 1.4043436253115652e-05
Train Loss: 0.6214, Train Acc: 0.6587, Train f1: 0.6090, Train Precision: 0.5773, Train Recall: 0.6445, Train AUC: 0.6941
Valitadion Loss: 0.6462, Validation Acc: 0.6434, Vall f1: 0.6136, Val Precision: 0.5586, Val Recall: 0.6807, Val AUC: 0.6707
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.46it/s]





































 98%|████████████████████████████████████████████████████████████████████████████████  | 82/84 [01:15<00:01,  1.11it/s]
New threshold is 0.4532017111778259
train F1 is 0.6287367343902588
100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:16<00:00,  1.10it/s]



 76%|██████████████████████████████████████████████████████████████▍                   | 16/21 [00:06<00:02,  2.40it/s]
New threshold is 0.4415181875228882
val F1 is 0.6039215922355652
Epoch 8/40, learning rate: 1.2486899247303603e-05
Train Loss: 0.6108, Train Acc: 0.6698, Train f1: 0.6287, Train Precision: 0.5863, Train Recall: 0.6778, Train AUC: 0.7003
Valitadion Loss: 0.6443, Validation Acc: 0.6469, Vall f1: 0.6039, Val Precision: 0.5662, Val Recall: 0.6471, Val AUC: 0.6761
100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.47it/s]





































 95%|██████████████████████████████████████████████████████████████████████████████    | 80/84 [01:13<00:03,  1.09it/s]
New threshold is 0.4594074785709381
train F1 is 0.6571428775787354

100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [01:16<00:00,  1.10it/s]


 71%|██████████████████████████████████████████████████████████▌                       | 15/21 [00:06<00:02,  2.26it/s]
Traceback (most recent call last):
  File "C:\Users\marcb\OneDrive\Desktop\mberghouse\Mammo_classification_scripts\cbisddsm_classification_300x500_old.py", line 567, in <module>
    model = train_model(model, model_name, criterion, optimizer, scheduler, num_epochs=epochs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marcb\OneDrive\Desktop\mberghouse\Mammo_classification_scripts\cbisddsm_classification_300x500_old.py", line 164, in train_model
    all_outputs = torch.cat((all_outputs, outputs.to('cpu')))
                                          ^^^^^^^^^^^^^^^^^
KeyboardInterrupt