stem.conv.weight
stem.conv.bias
stem.norm.weight
stem.norm.bias
stages.0.blocks.0.0.cpe1.proj.weight
stages.0.blocks.0.0.cpe1.proj.bias
stages.0.blocks.0.0.norm1.weight
stages.0.blocks.0.0.norm1.bias
stages.0.blocks.0.0.attn.qkv.weight
stages.0.blocks.0.0.attn.qkv.bias
stages.0.blocks.0.0.attn.proj.weight
stages.0.blocks.0.0.attn.proj.bias
stages.0.blocks.0.0.cpe2.proj.weight
stages.0.blocks.0.0.cpe2.proj.bias
stages.0.blocks.0.0.norm2.weight
stages.0.blocks.0.0.norm2.bias
stages.0.blocks.0.0.mlp.fc1.weight
stages.0.blocks.0.0.mlp.fc1.bias
stages.0.blocks.0.0.mlp.fc2.weight
stages.0.blocks.0.0.mlp.fc2.bias
stages.0.blocks.0.1.cpe1.proj.weight
stages.0.blocks.0.1.cpe1.proj.bias
stages.0.blocks.0.1.norm1.weight
stages.0.blocks.0.1.norm1.bias
stages.0.blocks.0.1.attn.qkv.weight
stages.0.blocks.0.1.attn.qkv.bias
stages.0.blocks.0.1.attn.proj.weight
stages.0.blocks.0.1.attn.proj.bias
stages.0.blocks.0.1.cpe2.proj.weight
stages.0.blocks.0.1.cpe2.proj.bias
stages.0.blocks.0.1.norm2.weight
stages.0.blocks.0.1.norm2.bias
stages.0.blocks.0.1.mlp.fc1.weight
stages.0.blocks.0.1.mlp.fc1.bias
stages.0.blocks.0.1.mlp.fc2.weight
stages.0.blocks.0.1.mlp.fc2.bias
stages.1.downsample.norm.weight
stages.1.downsample.norm.bias
stages.1.downsample.conv.weight
stages.1.downsample.conv.bias
stages.1.blocks.0.0.cpe1.proj.weight
stages.1.blocks.0.0.cpe1.proj.bias
stages.1.blocks.0.0.norm1.weight
stages.1.blocks.0.0.norm1.bias
stages.1.blocks.0.0.attn.qkv.weight
stages.1.blocks.0.0.attn.qkv.bias
stages.1.blocks.0.0.attn.proj.weight
stages.1.blocks.0.0.attn.proj.bias
stages.1.blocks.0.0.cpe2.proj.weight
stages.1.blocks.0.0.cpe2.proj.bias
stages.1.blocks.0.0.norm2.weight
stages.1.blocks.0.0.norm2.bias
stages.1.blocks.0.0.mlp.fc1.weight
stages.1.blocks.0.0.mlp.fc1.bias
stages.1.blocks.0.0.mlp.fc2.weight
stages.1.blocks.0.0.mlp.fc2.bias
stages.1.blocks.0.1.cpe1.proj.weight
stages.1.blocks.0.1.cpe1.proj.bias
stages.1.blocks.0.1.norm1.weight
stages.1.blocks.0.1.norm1.bias
stages.1.blocks.0.1.attn.qkv.weight
stages.1.blocks.0.1.attn.qkv.bias
stages.1.blocks.0.1.attn.proj.weight
stages.1.blocks.0.1.attn.proj.bias
stages.1.blocks.0.1.cpe2.proj.weight
stages.1.blocks.0.1.cpe2.proj.bias
stages.1.blocks.0.1.norm2.weight
stages.1.blocks.0.1.norm2.bias
stages.1.blocks.0.1.mlp.fc1.weight
stages.1.blocks.0.1.mlp.fc1.bias
stages.1.blocks.0.1.mlp.fc2.weight
stages.1.blocks.0.1.mlp.fc2.bias
stages.2.downsample.norm.weight
stages.2.downsample.norm.bias
stages.2.downsample.conv.weight
stages.2.downsample.conv.bias
stages.2.blocks.0.0.cpe1.proj.weight
stages.2.blocks.0.0.cpe1.proj.bias
stages.2.blocks.0.0.norm1.weight
stages.2.blocks.0.0.norm1.bias
stages.2.blocks.0.0.attn.qkv.weight
stages.2.blocks.0.0.attn.qkv.bias
stages.2.blocks.0.0.attn.proj.weight
stages.2.blocks.0.0.attn.proj.bias
stages.2.blocks.0.0.cpe2.proj.weight
stages.2.blocks.0.0.cpe2.proj.bias
stages.2.blocks.0.0.norm2.weight
stages.2.blocks.0.0.norm2.bias
stages.2.blocks.0.0.mlp.fc1.weight
stages.2.blocks.0.0.mlp.fc1.bias
stages.2.blocks.0.0.mlp.fc2.weight
stages.2.blocks.0.0.mlp.fc2.bias
stages.2.blocks.0.1.cpe1.proj.weight
stages.2.blocks.0.1.cpe1.proj.bias
stages.2.blocks.0.1.norm1.weight
stages.2.blocks.0.1.norm1.bias
stages.2.blocks.0.1.attn.qkv.weight
stages.2.blocks.0.1.attn.qkv.bias
stages.2.blocks.0.1.attn.proj.weight
stages.2.blocks.0.1.attn.proj.bias
stages.2.blocks.0.1.cpe2.proj.weight
stages.2.blocks.0.1.cpe2.proj.bias
stages.2.blocks.0.1.norm2.weight
stages.2.blocks.0.1.norm2.bias
stages.2.blocks.0.1.mlp.fc1.weight
stages.2.blocks.0.1.mlp.fc1.bias
stages.2.blocks.0.1.mlp.fc2.weight
stages.2.blocks.0.1.mlp.fc2.bias
stages.2.blocks.1.0.cpe1.proj.weight
stages.2.blocks.1.0.cpe1.proj.bias
stages.2.blocks.1.0.norm1.weight
stages.2.blocks.1.0.norm1.bias
stages.2.blocks.1.0.attn.qkv.weight
stages.2.blocks.1.0.attn.qkv.bias
stages.2.blocks.1.0.attn.proj.weight
stages.2.blocks.1.0.attn.proj.bias
stages.2.blocks.1.0.cpe2.proj.weight
stages.2.blocks.1.0.cpe2.proj.bias
stages.2.blocks.1.0.norm2.weight
stages.2.blocks.1.0.norm2.bias
stages.2.blocks.1.0.mlp.fc1.weight
stages.2.blocks.1.0.mlp.fc1.bias
stages.2.blocks.1.0.mlp.fc2.weight
stages.2.blocks.1.0.mlp.fc2.bias
stages.2.blocks.1.1.cpe1.proj.weight
stages.2.blocks.1.1.cpe1.proj.bias
stages.2.blocks.1.1.norm1.weight
stages.2.blocks.1.1.norm1.bias
stages.2.blocks.1.1.attn.qkv.weight
stages.2.blocks.1.1.attn.qkv.bias
stages.2.blocks.1.1.attn.proj.weight
stages.2.blocks.1.1.attn.proj.bias
stages.2.blocks.1.1.cpe2.proj.weight
stages.2.blocks.1.1.cpe2.proj.bias
stages.2.blocks.1.1.norm2.weight
stages.2.blocks.1.1.norm2.bias
stages.2.blocks.1.1.mlp.fc1.weight
stages.2.blocks.1.1.mlp.fc1.bias
stages.2.blocks.1.1.mlp.fc2.weight
stages.2.blocks.1.1.mlp.fc2.bias
stages.2.blocks.2.0.cpe1.proj.weight
stages.2.blocks.2.0.cpe1.proj.bias
stages.2.blocks.2.0.norm1.weight
stages.2.blocks.2.0.norm1.bias
stages.2.blocks.2.0.attn.qkv.weight
stages.2.blocks.2.0.attn.qkv.bias
stages.2.blocks.2.0.attn.proj.weight
stages.2.blocks.2.0.attn.proj.bias
stages.2.blocks.2.0.cpe2.proj.weight
stages.2.blocks.2.0.cpe2.proj.bias
stages.2.blocks.2.0.norm2.weight
stages.2.blocks.2.0.norm2.bias
stages.2.blocks.2.0.mlp.fc1.weight
stages.2.blocks.2.0.mlp.fc1.bias
stages.2.blocks.2.0.mlp.fc2.weight
stages.2.blocks.2.0.mlp.fc2.bias
stages.2.blocks.2.1.cpe1.proj.weight
stages.2.blocks.2.1.cpe1.proj.bias
stages.2.blocks.2.1.norm1.weight
stages.2.blocks.2.1.norm1.bias
stages.2.blocks.2.1.attn.qkv.weight
stages.2.blocks.2.1.attn.qkv.bias
stages.2.blocks.2.1.attn.proj.weight
stages.2.blocks.2.1.attn.proj.bias
stages.2.blocks.2.1.cpe2.proj.weight
stages.2.blocks.2.1.cpe2.proj.bias
stages.2.blocks.2.1.norm2.weight
stages.2.blocks.2.1.norm2.bias
stages.2.blocks.2.1.mlp.fc1.weight
stages.2.blocks.2.1.mlp.fc1.bias
stages.2.blocks.2.1.mlp.fc2.weight
stages.2.blocks.2.1.mlp.fc2.bias
stages.3.downsample.norm.weight
stages.3.downsample.norm.bias
stages.3.downsample.conv.weight
stages.3.downsample.conv.bias
stages.3.blocks.0.0.cpe1.proj.weight
stages.3.blocks.0.0.cpe1.proj.bias
stages.3.blocks.0.0.norm1.weight
stages.3.blocks.0.0.norm1.bias
stages.3.blocks.0.0.attn.qkv.weight
stages.3.blocks.0.0.attn.qkv.bias
stages.3.blocks.0.0.attn.proj.weight
stages.3.blocks.0.0.attn.proj.bias
stages.3.blocks.0.0.cpe2.proj.weight
stages.3.blocks.0.0.cpe2.proj.bias
stages.3.blocks.0.0.norm2.weight
stages.3.blocks.0.0.norm2.bias
stages.3.blocks.0.0.mlp.fc1.weight
stages.3.blocks.0.0.mlp.fc1.bias
stages.3.blocks.0.0.mlp.fc2.weight
stages.3.blocks.0.0.mlp.fc2.bias
stages.3.blocks.0.1.cpe1.proj.weight
stages.3.blocks.0.1.cpe1.proj.bias
stages.3.blocks.0.1.norm1.weight
stages.3.blocks.0.1.norm1.bias
stages.3.blocks.0.1.attn.qkv.weight
stages.3.blocks.0.1.attn.qkv.bias
stages.3.blocks.0.1.attn.proj.weight
stages.3.blocks.0.1.attn.proj.bias
stages.3.blocks.0.1.cpe2.proj.weight
stages.3.blocks.0.1.cpe2.proj.bias
stages.3.blocks.0.1.norm2.weight
stages.3.blocks.0.1.norm2.bias
stages.3.blocks.0.1.mlp.fc1.weight
stages.3.blocks.0.1.mlp.fc1.bias
stages.3.blocks.0.1.mlp.fc2.weight
stages.3.blocks.0.1.mlp.fc2.bias
head.norm.weight
head.norm.bias
head.fc.weight
head.fc.bias
head.fc.bias
DaVit(
  (stem): Stem(
    (conv): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
    (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)
  )
  (stages): Sequential(
    (0): DaVitStage(
      (downsample): Identity()
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (1): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (2): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
        (1): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
        (2): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (3): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
  )
  (norm_pre): Identity()
  (head): NormMlpClassifierHead(
    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
    (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (pre_logits): Identity()
    (drop): Dropout(p=0.0, inplace=False)
    (fc): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=768, out_features=64, bias=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
      (3): Linear(in_features=64, out_features=1, bias=False)
      (4): Sigmoid()
    )
  )
)
Starting training...
--------------------
train for epoch 1





































 97%|███████████████████████████████████████████████████████████████████████████████▍  | 95/98 [01:15<00:02,  1.26it/s]
New threshold is 0.48849329352378845
train F1 is 0.4953191578388214
100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.27it/s]





 90%|██████████████████████████████████████████████████████████████████████████        | 28/31 [00:10<00:01,  2.74it/s]
New threshold is 0.4394911527633667
val F1 is 0.5066666603088379
Epoch 1/40, learning rate: 1.981543834798841e-05
Train Loss: 0.6943, Train Acc: 0.4932, Train f1: 0.4953, Train Precision: 0.4786, Train Recall: 0.5132, Train AUC: 0.4931
Valitadion Loss: 0.6756, Validation Acc: 0.5900, Vall f1: 0.5067, Val Precision: 0.4841, Val Recall: 0.5315, Val AUC: 0.6000
100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.81it/s]






































100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.27it/s]
New threshold is 0.48338282108306885
train F1 is 0.5122807025909424
val for epoch 2




 87%|███████████████████████████████████████████████████████████████████████▍          | 27/31 [00:09<00:01,  2.72it/s]
New threshold is 0.45678678154945374
val F1 is 0.5194805264472961
Epoch 2/40, learning rate: 1.926856599297291e-05
Train Loss: 0.6921, Train Acc: 0.5248, Train f1: 0.5123, Train Precision: 0.5096, Train Recall: 0.5150, Train AUC: 0.5224
Valitadion Loss: 0.6801, Validation Acc: 0.5900, Vall f1: 0.5195, Val Precision: 0.4848, Val Recall: 0.5594, Val AUC: 0.5895
100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.80it/s]






































 99%|█████████████████████████████████████████████████████████████████████████████████▏| 97/98 [01:16<00:00,  1.26it/s]
New threshold is 0.4704214930534363
train F1 is 0.5627632737159729
100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.27it/s]




 84%|████████████████████████████████████████████████████████████████████▊             | 26/31 [00:09<00:01,  2.74it/s]
New threshold is 0.5037333965301514
val F1 is 0.5307443141937256
Epoch 3/40, learning rate: 1.8379569269019054e-05
Train Loss: 0.6894, Train Acc: 0.5564, Train f1: 0.5628, Train Precision: 0.5387, Train Recall: 0.5891, Train AUC: 0.5474
Valitadion Loss: 0.6903, Validation Acc: 0.5983, Vall f1: 0.5307, Val Precision: 0.4940, Val Recall: 0.5734, Val AUC: 0.6037
100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.80it/s]






































 98%|████████████████████████████████████████████████████████████████████████████████▎ | 96/98 [01:16<00:01,  1.26it/s]
New threshold is 0.4845561981201172
train F1 is 0.517241358757019
100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.26it/s]





 97%|███████████████████████████████████████████████████████████████████████████████▎  | 30/31 [00:11<00:00,  2.72it/s]
New threshold is 0.5126935839653015
val F1 is 0.46689894795417786
Epoch 4/40, learning rate: 1.7181263118568742e-05
Train Loss: 0.6921, Train Acc: 0.5214, Train f1: 0.5172, Train Precision: 0.5059, Train Recall: 0.5291, Train AUC: 0.5276
Valitadion Loss: 0.6955, Validation Acc: 0.5762, Vall f1: 0.4669, Val Precision: 0.4653, Val Recall: 0.4685, Val AUC: 0.5502
100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.78it/s]





































 97%|███████████████████████████████████████████████████████████████████████████████▍  | 95/98 [01:15<00:02,  1.25it/s]
New threshold is 0.4898371994495392
train F1 is 0.5210871696472168
100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.27it/s]





 90%|██████████████████████████████████████████████████████████████████████████        | 28/31 [00:10<00:01,  2.73it/s]
New threshold is 0.4886798560619354
val F1 is 0.5153374075889587
Epoch 5/40, learning rate: 1.5717879816382145e-05
Train Loss: 0.6871, Train Acc: 0.5632, Train f1: 0.5211, Train Precision: 0.5560, Train Recall: 0.4903, Train AUC: 0.5785
Valitadion Loss: 0.6845, Validation Acc: 0.5623, Vall f1: 0.5153, Val Precision: 0.4590, Val Recall: 0.5874, Val AUC: 0.5840
100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.78it/s]






































 99%|█████████████████████████████████████████████████████████████████████████████████▏| 97/98 [01:17<00:00,  1.27it/s]
New threshold is 0.5020249485969543
train F1 is 0.5268022418022156
100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.26it/s]




 87%|███████████████████████████████████████████████████████████████████████▍          | 27/31 [00:09<00:01,  2.73it/s]
New threshold is 0.4496116638183594
val F1 is 0.5
Epoch 6/40, learning rate: 1.4043436253115652e-05
Train Loss: 0.6845, Train Acc: 0.5624, Train f1: 0.5268, Train Precision: 0.5534, Train Recall: 0.5026, Train AUC: 0.5828
Valitadion Loss: 0.6727, Validation Acc: 0.5623, Vall f1: 0.5000, Val Precision: 0.4566, Val Recall: 0.5524, Val AUC: 0.5956
100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.79it/s]






































 98%|████████████████████████████████████████████████████████████████████████████████▎ | 96/98 [01:16<00:01,  1.27it/s]
New threshold is 0.49885061383247375
train F1 is 0.5459411144256592
100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.26it/s]




100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.79it/s]
  0%|                                                                                           | 0/98 [00:00<?, ?it/s]
New threshold is 0.5014898777008057
val F1 is 0.5597667694091797
Epoch 7/40, learning rate: 1.2219740045906977e-05
Train Loss: 0.6863, Train Acc: 0.5650, Train f1: 0.5459, Train Precision: 0.5523, Train Recall: 0.5397, Train AUC: 0.5691
Valitadion Loss: 0.6867, Validation Acc: 0.5817, Vall f1: 0.5598, Val Precision: 0.4800, Val Recall: 0.6713, Val AUC: 0.6183






































 97%|███████████████████████████████████████████████████████████████████████████████▍  | 95/98 [01:15<00:02,  1.26it/s]
New threshold is 0.4915069341659546
train F1 is 0.5414847135543823
100%|██████████████████████████████████████████████████████████████████████████████████| 98/98 [01:17<00:00,  1.26it/s]





 90%|██████████████████████████████████████████████████████████████████████████        | 28/31 [00:10<00:01,  2.71it/s]
New threshold is 0.4756346344947815
val F1 is 0.5443037748336792
Epoch 8/40, learning rate: 1.0314108075075903e-05
Train Loss: 0.6899, Train Acc: 0.5513, Train f1: 0.5415, Train Precision: 0.5363, Train Recall: 0.5467, Train AUC: 0.5458
Valitadion Loss: 0.6775, Validation Acc: 0.6011, Vall f1: 0.5443, Val Precision: 0.4971, Val Recall: 0.6014, Val AUC: 0.6223
100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:11<00:00,  2.76it/s]












 32%|█████████████████████████▉                                                        | 31/98 [00:25<00:55,  1.21it/s]
Traceback (most recent call last):
  File "C:\Users\marcb\OneDrive\Desktop\mberghouse\Mammo_classification_scripts\cbisddsm_classification_300x500_old.py", line 566, in <module>
    model = train_model(model, model_name, criterion, optimizer, scheduler, num_epochs=epochs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marcb\OneDrive\Desktop\mberghouse\Mammo_classification_scripts\cbisddsm_classification_300x500_old.py", line 174, in train_model
    running_loss += loss.item()
                    ^^^^^^^^^^^
KeyboardInterrupt