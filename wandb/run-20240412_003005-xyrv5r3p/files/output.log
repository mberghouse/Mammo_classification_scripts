
  0%|                                                                                           | 0/98 [00:00<?, ?it/s]
stem.conv.weight
stem.conv.bias
stem.norm.weight
stem.norm.bias
stages.0.blocks.0.0.cpe1.proj.weight
stages.0.blocks.0.0.cpe1.proj.bias
stages.0.blocks.0.0.norm1.weight
stages.0.blocks.0.0.norm1.bias
stages.0.blocks.0.0.attn.qkv.weight
stages.0.blocks.0.0.attn.qkv.bias
stages.0.blocks.0.0.attn.proj.weight
stages.0.blocks.0.0.attn.proj.bias
stages.0.blocks.0.0.cpe2.proj.weight
stages.0.blocks.0.0.cpe2.proj.bias
stages.0.blocks.0.0.norm2.weight
stages.0.blocks.0.0.norm2.bias
stages.0.blocks.0.0.mlp.fc1.weight
stages.0.blocks.0.0.mlp.fc1.bias
stages.0.blocks.0.0.mlp.fc2.weight
stages.0.blocks.0.0.mlp.fc2.bias
stages.0.blocks.0.1.cpe1.proj.weight
stages.0.blocks.0.1.cpe1.proj.bias
stages.0.blocks.0.1.norm1.weight
stages.0.blocks.0.1.norm1.bias
stages.0.blocks.0.1.attn.qkv.weight
stages.0.blocks.0.1.attn.qkv.bias
stages.0.blocks.0.1.attn.proj.weight
stages.0.blocks.0.1.attn.proj.bias
stages.0.blocks.0.1.cpe2.proj.weight
stages.0.blocks.0.1.cpe2.proj.bias
stages.0.blocks.0.1.norm2.weight
stages.0.blocks.0.1.norm2.bias
stages.0.blocks.0.1.mlp.fc1.weight
stages.0.blocks.0.1.mlp.fc1.bias
stages.0.blocks.0.1.mlp.fc2.weight
stages.0.blocks.0.1.mlp.fc2.bias
stages.1.downsample.norm.weight
stages.1.downsample.norm.bias
stages.1.downsample.conv.weight
stages.1.downsample.conv.bias
stages.1.blocks.0.0.cpe1.proj.weight
stages.1.blocks.0.0.cpe1.proj.bias
stages.1.blocks.0.0.norm1.weight
stages.1.blocks.0.0.norm1.bias
stages.1.blocks.0.0.attn.qkv.weight
stages.1.blocks.0.0.attn.qkv.bias
stages.1.blocks.0.0.attn.proj.weight
stages.1.blocks.0.0.attn.proj.bias
stages.1.blocks.0.0.cpe2.proj.weight
stages.1.blocks.0.0.cpe2.proj.bias
stages.1.blocks.0.0.norm2.weight
stages.1.blocks.0.0.norm2.bias
stages.1.blocks.0.0.mlp.fc1.weight
stages.1.blocks.0.0.mlp.fc1.bias
stages.1.blocks.0.0.mlp.fc2.weight
stages.1.blocks.0.0.mlp.fc2.bias
stages.1.blocks.0.1.cpe1.proj.weight
stages.1.blocks.0.1.cpe1.proj.bias
stages.1.blocks.0.1.norm1.weight
stages.1.blocks.0.1.norm1.bias
stages.1.blocks.0.1.attn.qkv.weight
stages.1.blocks.0.1.attn.qkv.bias
stages.1.blocks.0.1.attn.proj.weight
stages.1.blocks.0.1.attn.proj.bias
stages.1.blocks.0.1.cpe2.proj.weight
stages.1.blocks.0.1.cpe2.proj.bias
stages.1.blocks.0.1.norm2.weight
stages.1.blocks.0.1.norm2.bias
stages.1.blocks.0.1.mlp.fc1.weight
stages.1.blocks.0.1.mlp.fc1.bias
stages.1.blocks.0.1.mlp.fc2.weight
stages.1.blocks.0.1.mlp.fc2.bias
stages.2.downsample.norm.weight
stages.2.downsample.norm.bias
stages.2.downsample.conv.weight
stages.2.downsample.conv.bias
stages.2.blocks.0.0.cpe1.proj.weight
stages.2.blocks.0.0.cpe1.proj.bias
stages.2.blocks.0.0.norm1.weight
stages.2.blocks.0.0.norm1.bias
stages.2.blocks.0.0.attn.qkv.weight
stages.2.blocks.0.0.attn.qkv.bias
stages.2.blocks.0.0.attn.proj.weight
stages.2.blocks.0.0.attn.proj.bias
stages.2.blocks.0.0.cpe2.proj.weight
stages.2.blocks.0.0.cpe2.proj.bias
stages.2.blocks.0.0.norm2.weight
stages.2.blocks.0.0.norm2.bias
stages.2.blocks.0.0.mlp.fc1.weight
stages.2.blocks.0.0.mlp.fc1.bias
stages.2.blocks.0.0.mlp.fc2.weight
stages.2.blocks.0.0.mlp.fc2.bias
stages.2.blocks.0.1.cpe1.proj.weight
stages.2.blocks.0.1.cpe1.proj.bias
stages.2.blocks.0.1.norm1.weight
stages.2.blocks.0.1.norm1.bias
stages.2.blocks.0.1.attn.qkv.weight
stages.2.blocks.0.1.attn.qkv.bias
stages.2.blocks.0.1.attn.proj.weight
stages.2.blocks.0.1.attn.proj.bias
stages.2.blocks.0.1.cpe2.proj.weight
stages.2.blocks.0.1.cpe2.proj.bias
stages.2.blocks.0.1.norm2.weight
stages.2.blocks.0.1.norm2.bias
stages.2.blocks.0.1.mlp.fc1.weight
stages.2.blocks.0.1.mlp.fc1.bias
stages.2.blocks.0.1.mlp.fc2.weight
stages.2.blocks.0.1.mlp.fc2.bias
stages.2.blocks.1.0.cpe1.proj.weight
stages.2.blocks.1.0.cpe1.proj.bias
stages.2.blocks.1.0.norm1.weight
stages.2.blocks.1.0.norm1.bias
stages.2.blocks.1.0.attn.qkv.weight
stages.2.blocks.1.0.attn.qkv.bias
stages.2.blocks.1.0.attn.proj.weight
stages.2.blocks.1.0.attn.proj.bias
stages.2.blocks.1.0.cpe2.proj.weight
stages.2.blocks.1.0.cpe2.proj.bias
stages.2.blocks.1.0.norm2.weight
stages.2.blocks.1.0.norm2.bias
stages.2.blocks.1.0.mlp.fc1.weight
stages.2.blocks.1.0.mlp.fc1.bias
stages.2.blocks.1.0.mlp.fc2.weight
stages.2.blocks.1.0.mlp.fc2.bias
stages.2.blocks.1.1.cpe1.proj.weight
stages.2.blocks.1.1.cpe1.proj.bias
stages.2.blocks.1.1.norm1.weight
stages.2.blocks.1.1.norm1.bias
stages.2.blocks.1.1.attn.qkv.weight
stages.2.blocks.1.1.attn.qkv.bias
stages.2.blocks.1.1.attn.proj.weight
stages.2.blocks.1.1.attn.proj.bias
stages.2.blocks.1.1.cpe2.proj.weight
stages.2.blocks.1.1.cpe2.proj.bias
stages.2.blocks.1.1.norm2.weight
stages.2.blocks.1.1.norm2.bias
stages.2.blocks.1.1.mlp.fc1.weight
stages.2.blocks.1.1.mlp.fc1.bias
stages.2.blocks.1.1.mlp.fc2.weight
stages.2.blocks.1.1.mlp.fc2.bias
stages.2.blocks.2.0.cpe1.proj.weight
stages.2.blocks.2.0.cpe1.proj.bias
stages.2.blocks.2.0.norm1.weight
stages.2.blocks.2.0.norm1.bias
stages.2.blocks.2.0.attn.qkv.weight
stages.2.blocks.2.0.attn.qkv.bias
stages.2.blocks.2.0.attn.proj.weight
stages.2.blocks.2.0.attn.proj.bias
stages.2.blocks.2.0.cpe2.proj.weight
stages.2.blocks.2.0.cpe2.proj.bias
stages.2.blocks.2.0.norm2.weight
stages.2.blocks.2.0.norm2.bias
stages.2.blocks.2.0.mlp.fc1.weight
stages.2.blocks.2.0.mlp.fc1.bias
stages.2.blocks.2.0.mlp.fc2.weight
stages.2.blocks.2.0.mlp.fc2.bias
stages.2.blocks.2.1.cpe1.proj.weight
stages.2.blocks.2.1.cpe1.proj.bias
stages.2.blocks.2.1.norm1.weight
stages.2.blocks.2.1.norm1.bias
stages.2.blocks.2.1.attn.qkv.weight
stages.2.blocks.2.1.attn.qkv.bias
stages.2.blocks.2.1.attn.proj.weight
stages.2.blocks.2.1.attn.proj.bias
stages.2.blocks.2.1.cpe2.proj.weight
stages.2.blocks.2.1.cpe2.proj.bias
stages.2.blocks.2.1.norm2.weight
stages.2.blocks.2.1.norm2.bias
stages.2.blocks.2.1.mlp.fc1.weight
stages.2.blocks.2.1.mlp.fc1.bias
stages.2.blocks.2.1.mlp.fc2.weight
stages.2.blocks.2.1.mlp.fc2.bias
stages.3.downsample.norm.weight
stages.3.downsample.norm.bias
stages.3.downsample.conv.weight
stages.3.downsample.conv.bias
stages.3.blocks.0.0.cpe1.proj.weight
stages.3.blocks.0.0.cpe1.proj.bias
stages.3.blocks.0.0.norm1.weight
stages.3.blocks.0.0.norm1.bias
stages.3.blocks.0.0.attn.qkv.weight
stages.3.blocks.0.0.attn.qkv.bias
stages.3.blocks.0.0.attn.proj.weight
stages.3.blocks.0.0.attn.proj.bias
stages.3.blocks.0.0.cpe2.proj.weight
stages.3.blocks.0.0.cpe2.proj.bias
stages.3.blocks.0.0.norm2.weight
stages.3.blocks.0.0.norm2.bias
stages.3.blocks.0.0.mlp.fc1.weight
stages.3.blocks.0.0.mlp.fc1.bias
stages.3.blocks.0.0.mlp.fc2.weight
stages.3.blocks.0.0.mlp.fc2.bias
stages.3.blocks.0.1.cpe1.proj.weight
stages.3.blocks.0.1.cpe1.proj.bias
stages.3.blocks.0.1.norm1.weight
stages.3.blocks.0.1.norm1.bias
stages.3.blocks.0.1.attn.qkv.weight
stages.3.blocks.0.1.attn.qkv.bias
stages.3.blocks.0.1.attn.proj.weight
stages.3.blocks.0.1.attn.proj.bias
stages.3.blocks.0.1.cpe2.proj.weight
stages.3.blocks.0.1.cpe2.proj.bias
stages.3.blocks.0.1.norm2.weight
stages.3.blocks.0.1.norm2.bias
stages.3.blocks.0.1.mlp.fc1.weight
stages.3.blocks.0.1.mlp.fc1.bias
stages.3.blocks.0.1.mlp.fc2.weight
stages.3.blocks.0.1.mlp.fc2.bias
head.norm.weight
head.norm.bias
head.fc.weight
head.fc.bias
head.fc.bias
DaVit(
  (stem): Stem(
    (conv): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
    (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)
  )
  (stages): Sequential(
    (0): DaVitStage(
      (downsample): Identity()
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (proj): Linear(in_features=96, out_features=96, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
              (act): Identity()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (1): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((96,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (proj): Linear(in_features=192, out_features=192, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
              (act): Identity()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (2): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((192,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
        (1): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
        (2): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (proj): Linear(in_features=384, out_features=384, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
              (act): Identity()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
    (3): DaVitStage(
      (downsample): Downsample(
        (norm): LayerNorm2d((384,), eps=1e-05, elementwise_affine=True)
        (conv): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
      )
      (blocks): Sequential(
        (0): Sequential(
          (0): SpatialBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (softmax): Softmax(dim=-1)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
          (1): ChannelBlock(
            (cpe1): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ChannelAttention(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (drop_path1): Identity()
            (cpe2): ConvPosEnc(
              (proj): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
              (act): Identity()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (drop_path2): Identity()
          )
        )
      )
    )
  )
  (norm_pre): Identity()
  (head): NormMlpClassifierHead(
    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
    (norm): LayerNorm2d((768,), eps=1e-05, elementwise_affine=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
    (pre_logits): Identity()
    (drop): Dropout(p=0.0, inplace=False)
    (fc): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=768, out_features=64, bias=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
      (3): Linear(in_features=64, out_features=1, bias=False)
      (4): Sigmoid()
    )
  )
)
Starting training...
--------------------

























 62%|███████████████████████████████████████████████████                               | 61/98 [00:55<00:33,  1.10it/s]
Traceback (most recent call last):
  File "C:\Users\marcb\OneDrive\Desktop\mberghouse\Mammo_classification_scripts\cbisddsm_classification_300x500_old.py", line 566, in <module>
    model = train_model(model, model_name, criterion, optimizer, scheduler, num_epochs=epochs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\marcb\OneDrive\Desktop\mberghouse\Mammo_classification_scripts\cbisddsm_classification_300x500_old.py", line 176, in train_model
    gc.collect()
KeyboardInterrupt