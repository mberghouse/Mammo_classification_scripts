{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframes for image loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.optim import Adam, SGD\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset, Dataset, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torchvision import models\n",
    "from torchmetrics.classification import BinaryF1Score, BinaryPrecision, BinaryRecall, BinaryAccuracy, BinaryROC, BinaryAUROC\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# visualisation\n",
    "import seaborn as sns\n",
    "\n",
    "# helpers\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "from enum import Enum\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "metadata_list=['mass_case_description_test_set','mass_case_description_train_set']\n",
    "#'mass_case_description_test_set','mass_case_description_train_set',\n",
    "df_list=[]\n",
    "for j in range(len(metadata_list)):\n",
    "    print (metadata_list[j])\n",
    "    df=pd.read_csv('/home/mbadhan/Desktop/mberghouse/'+metadata_list[j]+'.csv')\n",
    "    print (len(df))\n",
    "    \n",
    "    fname=[]\n",
    "\n",
    "    df=df.rename(columns={\"file path\": \"filename\",\"pathology\":\"class\", 'image file path':'filename','cropped image file path':'patch_filename'})\n",
    "###Remove multiple-counted images for whole image\n",
    "    for k in range(len(df)):\n",
    "        fname.append(df['filename'].loc[k])\n",
    "        if k>0:\n",
    "            if fname[k] == fname[k-1]:\n",
    "                df.drop(k, inplace=True)\n",
    "    print ('df length after removal of repeats: ', len(df))\n",
    "    \n",
    "    \n",
    "    nan_count=0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "### For whole images\n",
    "        df.filename.iloc[i]='/home/mbadhan/Desktop/mberghouse/CBIS-DDSM-preprocessed/600x1000_v6/'+df.filename.iloc[i].rsplit('/',3)[0]+'/1-1.png'\n",
    "        #df.filename[i]='F:/CBIS_DDSM_redo/manifest-ZkhPvrLo5216730872708713142/CBIS-DDSM-png/'+df.filename[i].rsplit('/',3)[0]+'/1-1.png'\n",
    "        if 'MALIGNANT' in df['class'].iloc[i]:\n",
    "            df['class'].iloc[i]=1\n",
    "        else:\n",
    "            df['class'].iloc[i]=0\n",
    "    df_list.append(df)\n",
    "    \n",
    "### For patches\n",
    "        ##i=i-nan_count\n",
    "       ## df.filename.iloc[i]='C:/Users/marcb/Desktop/CBIS-DDSM-preprocessed/v4_3d/'+df.filename.iloc[i].rsplit('/',3)[0]+'/1-1.jpg'\n",
    "        ##df.filename[i]='F:/CBIS_DDSM_redo/manifest-ZkhPvrLo5216730872708713142/CBIS-DDSM-png/'+df.filename[i].rsplit('/',3)[0]+'/1-1.png'\n",
    "### For Masses\n",
    "#         try:\n",
    "#             path1 = 'F:/CBIS_DDSM/manifest-ZkhPvrLo5216730872708713142/CBIS-DDSM-mask/'+df.patch_filename.iloc[i].rsplit('/',3)[0]\n",
    "#             folder1=os.listdir(path1)[0]\n",
    "#             folder2=os.listdir(os.path.join(path1,folder1))[0]\n",
    "#             img = os.listdir(os.path.join(path1,folder1,folder2))\n",
    "#             im0_path=os.path.join(path1,folder1,folder2,img[0])\n",
    "#             im0=cv2.imread(im0_path)\n",
    "#             if len(img)>1:\n",
    "#                 if np.median(im0)==0:\n",
    "#                     img_path = os.path.join(path1,folder1,folder2,img[1])\n",
    "#                 else:\n",
    "#                     img_path = im0_path\n",
    "#             else:\n",
    "#                 if np.median(im0)==0:\n",
    "#                     img_path = np.nan\n",
    "#                     nan_count = nan_count+1\n",
    "#                 else:\n",
    "#                     img_path = im0_path\n",
    "#             df.patch_filename.iloc[i]=img_path\n",
    "#         except:\n",
    "#             path1 = 'F:/CBIS_DDSM/manifest-ZkhPvrLo5216730872708713142/cropped/'+df.patch_filename.iloc[i].rsplit('/',3)[0]\n",
    "#             folder1=os.listdir(path1)[0]\n",
    "#             folder2=os.listdir(os.path.join(path1,folder1))[0]\n",
    "#             img = os.listdir(os.path.join(path1,folder1,folder2))\n",
    "#             im0_path=os.path.join(path1,folder1,folder2,img[0])\n",
    "#             im0=cv2.imread(im0_path)\n",
    "#             if len(img)>1:               \n",
    "#                 if np.median(im0)==0:\n",
    "#                     img_path = os.path.join(path1,folder1,folder2,img[1])\n",
    "#                 else:\n",
    "#                     img_path = im0_path\n",
    "#             else:\n",
    "#                 if np.median(im0)==0:\n",
    "#                     img_path = np.nan\n",
    "#                     nan_count = nan_count+1\n",
    "#                 else:\n",
    "#                     img_path = im0_path\n",
    "#             df.patch_filename.iloc[i]=img_path\n",
    "#             #df.dropna(inplace=True)\n",
    "        \n",
    "#         if 'MALIGNANT' in df['class'].iloc[i]:\n",
    "#             df['class'].iloc[i]=1\n",
    "#         else:\n",
    "#             df['class'].iloc[i]=0\n",
    "#     df.dropna(inplace=True)\n",
    "#     df_list.append(df)\n",
    "#df_list\n",
    "# df_mass_test=df_list[0]\n",
    "# df_mass_train=df_list[1]\n",
    "df_calc_test=df_list[0]\n",
    "df_calc_train=df_list[1]\n",
    "\n",
    "# filenames=[]\n",
    "# labels=[]\n",
    "# for i in range(len(df_mass_train)):\n",
    "#     filenames.append(df_mass_train.filename.iloc[i])\n",
    "#     labels.append(df_mass_train['class'].iloc[i])\n",
    "\n",
    "# filenames_test=[]\n",
    "# labels_test=[]\n",
    "# for i in range(len(df_mass_test)):\n",
    "#     filenames_test.append(df_mass_test.filename.iloc[i])\n",
    "#     labels_test.append(df_mass_test['class'].iloc[i])\n",
    "    \n",
    "# filenames_calc=[]\n",
    "# labels_calc=[]\n",
    "# for i in range(len(df_calc_train)):\n",
    "#     filenames_calc.append(df_calc_train.filename.iloc[i])\n",
    "#     labels_calc.append(df_calc_train['class'].iloc[i])\n",
    "\n",
    "# filenames_test_calc=[]\n",
    "# labels_test_calc=[]\n",
    "# for i in range(len(df_calc_test)):\n",
    "#     filenames_test_calc.append(df_calc_test.filename.iloc[i])\n",
    "#     labels_test_calc.append(df_calc_test['class'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_calc=[]\n",
    "labels_calc=[]\n",
    "for i in range(len(df_calc_train)):\n",
    "    filenames_calc.append(df_calc_train.filename.iloc[i])\n",
    "    labels_calc.append(df_calc_train['class'].iloc[i])\n",
    "\n",
    "filenames_test_calc=[]\n",
    "labels_test_calc=[]\n",
    "for i in range(len(df_calc_test)):\n",
    "    filenames_test_calc.append(df_calc_test.filename.iloc[i])\n",
    "    labels_test_calc.append(df_calc_test['class'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "import numpy.matlib as np_mlb\n",
    "#torchvision.disable_beta_transforms_warning()\n",
    "#import torchvision.transforms.v2 as transforms\n",
    "\n",
    "#plt.ion()   # interactive mode\n",
    "\n",
    "##########################################\n",
    "##### HERE ARE THE AUGMENTATIONS!!! ######\n",
    "##########################################\n",
    "# affine_transfomer = T.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75))\n",
    "\n",
    "augmentator = transforms.Compose([\n",
    "    # input for augmentator is always PIL image\n",
    "    transforms.ToPILImage(),\n",
    "    \n",
    "    transforms.ColorJitter(brightness=(.6,1.2)),\n",
    "#     transforms.Pad(padding=(12,3)),\n",
    "    #torchvision.models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms(),\n",
    "#     transforms.RandomAdjustSharpness(1.5, p=0.5),\n",
    "#     transforms.RandomPhotometricDistort(),\n",
    "    #transforms.RandomZoomOut(fill = 0, side_range = (1, 1.3)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.RandomRotation(12),\n",
    "#     transforms.Resize(size=(500,300)),\n",
    "    #transforms.RandomAffine(degrees=(0,10),translate=(0.01, 0.02),  shear=(.5,.5)),\n",
    "    #transforms.GaussianBlur(kernel_size=(3, 3), sigma=(.5)),\n",
    "#     transforms.RandomEqualize(p=.1),\n",
    "#    transforms.RandomAutocontrast(),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.ConvertImageDtype(torch.float32),# return it as a tensor and transforms it to [0, 1]\n",
    "])\n",
    "\n",
    "small_aug = transforms.Compose([\n",
    "    # input for augmentator is always PIL image\n",
    "    transforms.ToPILImage(),\n",
    "    #transforms.Pad(10),\n",
    "    #torchvision.models.EfficientNet_V2_S_Weights.IMAGENET1K_V1.transforms()\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.v2.ConvertImageDtype(torch.float32)# return it as a tensor and transforms it to [0, 1]\n",
    "])\n",
    "\n",
    "def show_landmarks(image, label):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    if label==0:\n",
    "        classif='Benign'\n",
    "    else:\n",
    "        classif='Malignant'\n",
    "    print ('Class: ',classif)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "class CBISDataset(Dataset):\n",
    "    \"\"\"CBIS-DDSM dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, labels, filenames, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        fname = self.filenames[idx]\n",
    "        image = cv2.imread(fname)\n",
    "#         image = cv2.resize(image, (344,344))\n",
    "#         image = (((image-np.min(image))/(np.max(image)-np.min(image)))*255).astype('uint8')\n",
    "#         clahe = cv2.createCLAHE(clipLimit = 2,tileGridSize=(12,12))\n",
    "#         image = clahe.apply(image[:,:,0])\n",
    "#         image = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n",
    "       # image = torchvision.io.read_image(fname)#, torchvision.io.ImageReadMode.RGB)\n",
    "       # image = image.repeat([3, 1, 1])\n",
    "        label = self.labels[idx]    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames_calc.pop(375)\n",
    "# filenames_calc.pop(494)\n",
    "# filenames_calc.pop(705)\n",
    "# filenames_calc.pop(928)\n",
    "# filenames_calc.pop(928)\n",
    "# filenames_calc.pop(928)\n",
    "# labels_calc.pop(375)\n",
    "# labels_calc.pop(494)\n",
    "# labels_calc.pop(705)\n",
    "# labels_calc.pop(928)\n",
    "# labels_calc.pop(928)\n",
    "# labels_calc.pop(928)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames_calc.pop(520)\n",
    "# filenames_calc.pop(520)\n",
    "# filenames_calc.pop(838)\n",
    "# filenames_calc.pop(1107)\n",
    "# filenames_calc.pop(1107)\n",
    "\n",
    "# labels_calc.pop(520)\n",
    "# labels_calc.pop(520)\n",
    "# labels_calc.pop(838)\n",
    "# labels_calc.pop(1107)\n",
    "# labels_calc.pop(1107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CBISDataset(labels_calc,filenames_calc,transform=augmentator)\n",
    "\n",
    "val_pct = 0.1\n",
    "val_size = int(val_pct * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = CBISDataset(labels_test_calc,filenames_test_calc, transform=small_aug)\n",
    "val_size =  len(val_dataset)\n",
    "train_size = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "#try pin memory = False\n",
    "\n",
    "# Create data loaders.\n",
    "#train_dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "#                        shuffle=True)#, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False, pin_memory = True,num_workers=batch_size)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, pin_memory = True,num_workers=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle = True, pin_memory = True,num_workers=batch_size)\n",
    "\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    #img=X\n",
    "    break\n",
    "plt.imshow(X[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "# create class for earlystopping\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_loss = np.inf\n",
    "\n",
    "    def early_stop(self, loss):\n",
    "        if loss <= self.min_loss:\n",
    "            self.min_loss = loss\n",
    "            self.counter = 0\n",
    "        elif loss > (self.min_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def BCELoss_class_weighted(weights):\n",
    "    \"\"\"\n",
    "    weights[0] is weight for class 0 (negative class)\n",
    "    weights[1] is weight for class 1 (positive class)\n",
    "    \"\"\"\n",
    "    def loss(y_pred, target):\n",
    "        y_pred = torch.clamp(y_pred,min=1e-7,max=1-1e-7) # for numerical stability\n",
    "        bce = - weights[1] * target * torch.log(y_pred) - (1 - target) * weights[0] * torch.log(1 - y_pred)\n",
    "        return torch.mean(bce)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def find_optim_thres(fpr, tpr, thresholds):\n",
    "    optim_thres = thresholds[0]\n",
    "    inx = 0\n",
    "    min_dist = 1.0\n",
    "    for i in range(len(fpr)):\n",
    "        dist = np.linalg.norm(np.array([0.0, 1.0]) - np.array([fpr[i], tpr[i]]))\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            optim_thres = thresholds[i]\n",
    "            inx = i\n",
    "            \n",
    "    return optim_thres, inx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    scheduler1=scheduler[1]\n",
    "    scheduler=scheduler[0]\n",
    "    since = time.time()\n",
    "    metricf1 = BinaryF1Score()\n",
    "    precision = BinaryPrecision()\n",
    "    recall = BinaryRecall()\n",
    "    accuracy = BinaryAccuracy()\n",
    "    roc = BinaryROC()\n",
    "    auc = BinaryAUROC()\n",
    "    best_model_wts = model.state_dict()\n",
    "    stop_count = 0\n",
    "    best_f1 = -1.0\n",
    "    train_metrics = {'loss' : [], 'acc' : [], 'f1': [], 'precision': [], 'recall': [], 'auc': []}\n",
    "    val_metrics = {'loss' : [], 'acc' : [], 'f1': [], 'precision': [], 'recall': [], 'auc': []}\n",
    "    # inital threshold for first epoch, it will change afterwards\n",
    "    threshold = 0.5\n",
    "    sched_steps=[]\n",
    "    print('Starting training...')\n",
    "    print('-' * 20)\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            # empty 'all' tensors for saving\n",
    "            # for calculating aoc at the end of epoch, and for calculating new threshold\n",
    "            all_outputs = torch.Tensor([])\n",
    "            all_labels = torch.Tensor([])\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            n_samples = 0\n",
    "            n_correct = 0\n",
    "            running_f1 = 0.0\n",
    "            # Iterate over data.\n",
    "            print(f'{phase} for epoch {epoch + 1}')\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                labels = torch.unsqueeze(torch.tensor(labels), 1).to(dtype=torch.float)               \n",
    "                #labels=torch.tensor(labels)\n",
    "                inputs = inputs.float()\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    preds = (outputs > threshold).double()\n",
    "                    #print(all_outputs)\n",
    "                    #print(outputs)\n",
    "                    # concatenating all outputs and labels for calculation aoc and new threshold\n",
    "                    all_outputs = torch.cat((all_outputs, outputs.to('cpu')))\n",
    "                    all_labels = torch.cat((all_labels, labels.to('cpu')))                  \n",
    "                    #print(labels)\n",
    "                    # _, preds = torch.max(outputs, 1)\n",
    "#                     loss=criterion(model(inputs)[model(inputs)>0], labels[model(inputs)>0])\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        scheduler1.step()\n",
    "#                         start=scheduler.state_dict()\n",
    "                        sched_steps.append(optimizer.param_groups[0]['lr'])\n",
    "#                         if epoch == 18:\n",
    "#                             lower=scheduler.state_dict()\n",
    "#                         if epoch ==40:\n",
    "# #                             optimizer = torch.optim.NAdam(model.parameters(), lr=1e-5, weight_decay=.1)\n",
    "# #                             scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 1600, T_mult=2, eta_min=1e-10, last_epoch=- 1, verbose=False)\n",
    "\n",
    "#                             for name, param in model.named_parameters():\n",
    "#                                 param.requires_grad = True\n",
    "#                         if epoch ==60:\n",
    "#                             optimizer.param_groups[0]['lr']=1e-5\n",
    "#                             scheduler.load_state_dict(start)\n",
    "\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                # collect any unused memmory\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()           \n",
    "            # statistics\n",
    "            epoch_loss = running_loss / len(dataloaders[phase])            \n",
    "            # find true positive and false positive rates for ROC curve\n",
    "            #print ('outputs: ', all_outputs, 'labels', all_labels)\n",
    "            all_labels=all_labels.to(dtype=torch.long)\n",
    "            fpr, tpr, thresholds = roc(all_outputs, all_labels)\n",
    "            epoch_auc = auc(all_outputs, all_labels)\n",
    "            # find new threshold\n",
    "            threshold, _ = find_optim_thres(fpr, tpr, thresholds)\n",
    "            print(f'New threshold is {threshold}')\n",
    "            # calculate metrics using new optimized threshold\n",
    "            epoch_f1 = metricf1(all_outputs > threshold, all_labels)\n",
    "            epoch_acc = accuracy(all_outputs > threshold, all_labels)\n",
    "            epoch_precision = precision(all_outputs > threshold, all_labels)\n",
    "            epoch_recall = recall(all_outputs > threshold, all_labels)\n",
    "            print(f'{phase} F1 is {epoch_f1}')            \n",
    "            # save all of the statistics for latter analysis\n",
    "            if phase == 'train':\n",
    "                wandb.log({\"train_acc\": epoch_acc, \"train_loss\": epoch_loss, \"train_f1\": epoch_f1, \"train_precision\": epoch_precision\n",
    "                         , \"train_recall\": epoch_recall, \"train_auc\": epoch_auc})\n",
    "                train_metrics['loss'].append(epoch_loss)\n",
    "                train_metrics['acc'].append(epoch_acc)\n",
    "                train_metrics['f1'].append(epoch_f1)\n",
    "                train_metrics['precision'].append(epoch_precision)\n",
    "                train_metrics['recall'].append(epoch_recall)\n",
    "                train_metrics['auc'].append(epoch_auc)\n",
    "            else:\n",
    "                wandb.log({\"val_acc\": epoch_acc, \"val_loss\": epoch_loss, \"val_f1\": epoch_f1, \"val_precision\": epoch_precision\n",
    "                         , \"val_recall\": epoch_recall, \"val_auc\": epoch_auc})\n",
    "                val_metrics['loss'].append(epoch_loss)\n",
    "                val_metrics['acc'].append(epoch_acc)\n",
    "                val_metrics['f1'].append(epoch_f1)\n",
    "                val_metrics['precision'].append(epoch_precision)\n",
    "                val_metrics['recall'].append(epoch_recall)\n",
    "                val_metrics['auc'].append(epoch_auc)\n",
    "\n",
    "            # deep copy the model\n",
    "                if val_metrics['f1'][-1] > best_f1:\n",
    "                    best_f1 = val_metrics['f1'][-1]\n",
    "                    best_model_wts = model.state_dict()\n",
    "                    checkpoint['threshold'] = threshold\n",
    "                    torch.save(checkpoint, 'checkpoint.pth')\n",
    "               \n",
    "        # cant be formated in strin g\n",
    "        tr_loss, tr_acc, tr_f1, tr_prec, tr_rec, tr_auc = train_metrics['loss'][-1], train_metrics['acc'][-1],  train_metrics['f1'][-1], train_metrics['precision'][-1], train_metrics['recall'][-1], train_metrics['auc'][-1]\n",
    "        val_loss, val_acc, val_f1, val_prec, val_rec, val_auc = val_metrics['loss'][-1], val_metrics['acc'][-1], val_metrics['f1'][-1], val_metrics['precision'][-1], val_metrics['recall'][-1], val_metrics['auc'][-1]\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, learning rate: {lr}')\n",
    "        print(f'Train Loss: {tr_loss:.4f}, Train Acc: {tr_acc:.4f}, Train f1: {tr_f1:.4f}, Train Precision: {tr_prec:.4f}, Train Recall: {tr_rec:.4f}, Train AUC: {tr_auc:.4f}')\n",
    "        print(f'Valitadion Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}, Vall f1: {val_f1:.4f}, Val Precision: {val_prec:.4f}, Val Recall: {val_rec:.4f}, Val AUC: {val_auc:.4f}')       \n",
    "#         if epoch == 0:\n",
    "#             val_f1_best=val_f1\n",
    "#         else:\n",
    "#             if val_f1 > val_f1_best:\n",
    "#                 val_f1_best=val_f1\n",
    "#                 stop_count = 0\n",
    "#             else:\n",
    "#                 stop_count = stop_count + 1\n",
    "        \n",
    "#         if stop_count == 160:\n",
    "#             break\n",
    "                \n",
    "#         if earlystoper.early_stop(val_f1):\n",
    "#             break       \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val auc: {best_f1:4f}')\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model, train_metrics, val_metrics, sched_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,criterion,dataloader, threshold=.5):\n",
    "    device='cuda'\n",
    "    since = time.time()\n",
    "    metricf1 = BinaryF1Score()\n",
    "    precision = BinaryPrecision()\n",
    "    recall = BinaryRecall()\n",
    "    accuracy = BinaryAccuracy()\n",
    "    roc = BinaryROC()\n",
    "    auc = BinaryAUROC()\n",
    "    stop_count = 0\n",
    "    best_f1 = -1.0\n",
    "    test_metrics = {'loss' : [], 'acc' : [], 'f1': [], 'precision': [], 'recall': [], 'auc': []}\n",
    "    # inital threshold for first epoch, it will change afterwards\n",
    "#     threshold = 0.5\n",
    "    print('Starting testing...')\n",
    "    # empty 'all' tensors for saving\n",
    "    all_outputs = torch.Tensor([])\n",
    "    all_labels = torch.Tensor([])\n",
    "    model.eval()   # Set model to evaluate mode\n",
    "    running_loss = 0.0\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    running_f1 = 0.0\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        labels = torch.unsqueeze(torch.tensor(labels), 1).to(dtype=torch.float)               \n",
    "        #labels=torch.tensor(labels)\n",
    "        inputs = inputs.float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "        # zero the parameter gradients\n",
    "            outputs = model(inputs)\n",
    "            #preds = (outputs > threshold).double()\n",
    "            # concatenating all outputs and labels for calculation aoc and new threshold\n",
    "            all_outputs = torch.cat((all_outputs, outputs.to('cpu')))\n",
    "            all_labels = torch.cat((all_labels, labels.to('cpu')))                  \n",
    "            #print(labels)\n",
    "            # _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        #n_correct += (preds == labels).sum().item()\n",
    "        # collect any unused memmory\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  \n",
    "        \n",
    "    # statistics\n",
    "    epoch_loss = running_loss / len(dataloader)            \n",
    "    # find true positive and false positive rates for ROC curve\n",
    "    #print ('outputs: ', all_outputs, 'labels', all_labels)\n",
    "    all_labels=all_labels.to(dtype=torch.long)\n",
    "    fpr, tpr, thresholds = roc(all_outputs, all_labels)\n",
    "    epoch_auc = auc(all_outputs, all_labels)\n",
    "    # find new threshold\n",
    "    threshold, _ = find_optim_thres(fpr, tpr, thresholds)\n",
    "    print(f'New threshold is {threshold}')\n",
    "    # calculate metrics using new optimized threshold\n",
    "    epoch_f1 = metricf1(all_outputs > threshold, all_labels)\n",
    "    epoch_acc = accuracy(all_outputs > threshold, all_labels)\n",
    "    epoch_precision = precision(all_outputs > threshold, all_labels)\n",
    "    epoch_recall = recall(all_outputs > threshold, all_labels)\n",
    "    # save all of the statistics for latter analysis\n",
    "    test_metrics['loss'].append(epoch_loss)\n",
    "    test_metrics['acc'].append(epoch_acc)\n",
    "    test_metrics['f1'].append(epoch_f1)\n",
    "    test_metrics['precision'].append(epoch_precision)\n",
    "    test_metrics['recall'].append(epoch_recall)\n",
    "    test_metrics['auc'].append(epoch_auc)               \n",
    "    time_elapsed = time.time() - since\n",
    "    wandb.log({\"test_acc\": epoch_acc, \"test_loss\": epoch_loss, \"test_f1\": epoch_f1, \"test_precision\": epoch_precision\n",
    "                         , \"test_recall\": epoch_recall, \"test_auc\": epoch_auc})\n",
    "    print(f'Inference complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'F1 Score = : {epoch_f1:4f}')\n",
    "    print(f'AUC Score = : {epoch_auc:4f}')\n",
    "    print(f'Acc Score = : {epoch_acc:4f}')\n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model From Timm and Reset Classifier. SE and CBAM Models are created by changing the source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# import timm\n",
    "# model = timm.create_model('densenet121', pretrained=True)\n",
    "\n",
    "# num_in_features = model.get_classifier().in_features\n",
    "\n",
    "# # Replace the existing classifier. It's named: classifier\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Dropout(.3),\n",
    "#     nn.Linear(in_features=num_in_features, out_features=64, bias=False),\n",
    "#     nn.LeakyReLU(.1,inplace=True),\n",
    "#     #nn.Dropout(.5),\n",
    "#     nn.Linear(in_features=64, out_features=1, bias=False),\n",
    "#     nn.Sigmoid())\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegNet(\n",
       "  (stem): ConvNormAct(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNormAct2d(\n",
       "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (drop): Identity()\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (s1): RegStage(\n",
       "    (b1): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(32, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(168, 168, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=3, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(168, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(8, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): ConvNormAct(\n",
       "        (conv): Conv2d(32, 168, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b2): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(168, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(168, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(42, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(168, 168, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       "  (s2): RegStage(\n",
       "    (b1): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(168, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=7, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(392, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(42, 392, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): ConvNormAct(\n",
       "        (conv): Conv2d(168, 392, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b2): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(392, 98, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(98, 392, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b3): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(392, 98, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(98, 392, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b4): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(392, 98, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(98, 392, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(392, 392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       "  (s3): RegStage(\n",
       "    (b1): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(392, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 98, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(98, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): ConvNormAct(\n",
       "        (conv): Conv2d(392, 784, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b2): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b3): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b4): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b5): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b6): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b7): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b8): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b9): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "    (b10): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(784, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 784, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(784, 784, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): Identity()\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       "  (s4): RegStage(\n",
       "    (b1): Bottleneck(\n",
       "      (conv1): ConvNormAct(\n",
       "        (conv): Conv2d(784, 1624, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): ConvNormAct(\n",
       "        (conv): Conv2d(1624, 1624, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=29, bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (se): SEModule(\n",
       "        (fc1): Conv2d(1624, 196, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): Identity()\n",
       "        (act): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(196, 1624, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate): Sigmoid()\n",
       "      )\n",
       "      (conv3): ConvNormAct(\n",
       "        (conv): Conv2d(1624, 1624, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (act3): ReLU()\n",
       "      (downsample): ConvNormAct(\n",
       "        (conv): Conv2d(784, 1624, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNormAct2d(\n",
       "          1624, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Identity()\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (fc): Linear(in_features=1624, out_features=1000, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model=my_resnet50(weights=\"IMAGENET1K_V2\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if ('sa' not in name)&('ca' not in name)&('classifier' not in name)&('se' not in name):\n",
    "#         param.requires_grad = False\n",
    "#     print(name, param.requires_grad)\n",
    "import timm\n",
    "model = timm.create_model('regnetx_064', pretrained=True)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad=False\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'se' in name:\n",
    "#         param.requires_grad=True\n",
    "# num_in_features = model.get_classifier().in_features\n",
    "\n",
    "# # Replace the existing classifier. It's named: classifier\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Dropout(dropout),\n",
    "#     nn.Linear(in_features=num_in_features, out_features=hidden_size, bias=False),\n",
    "#     nn.LeakyReLU(.1,inplace=True),\n",
    "#     #nn.Dropout(.5),\n",
    "#     nn.Linear(in_features=hidden_size, out_features=1, bias=False),\n",
    "#     nn.Sigmoid())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### gc.collect()\n",
    "import wandb\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "for i in range(4):\n",
    "    lr = np.random.uniform(4e-5,7e-5)\n",
    "    epochs = np.random.randint(25,26)\n",
    "    #epochs=30\n",
    "    hidden_size = 64\n",
    "    dropout = .1\n",
    "    \n",
    "\n",
    "    model = timm.create_model('eca_resnet50', pretrained=True)\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad=False\n",
    "\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if 'se' in name:\n",
    "#             param.requires_grad=True\n",
    "\n",
    "    num_in_features = model.get_classifier().in_features\n",
    "\n",
    "    # Replace the existing classifier. It's named: classifier\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(in_features=num_in_features, out_features=hidden_size, bias=False),\n",
    "        nn.LeakyReLU(.1,inplace=True),\n",
    "        #nn.Dropout(.5),\n",
    "        nn.Linear(in_features=hidden_size, out_features=1, bias=True),\n",
    "        nn.Sigmoid())\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=.001)\n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 580, T_mult=2, eta_min=1e-12, last_epoch=- 1, verbose=False)\n",
    "\n",
    "    scheduler1 = StepLR(optimizer, step_size=2000000, gamma=0.1)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    test_size=len(test_dataset)\n",
    "    dataloaders = {'train' : train_dataloader, 'val' : test_dataloader}\n",
    "    dataset_sizes = {'train': train_size, 'val' : test_size}\n",
    "    checkpoint = {'model': 'abc',\n",
    "              'state_dict': model.state_dict(),\n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "                 'threshold' : 0.5}\n",
    "    \n",
    "    wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"Model Testing\",\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": 'ECAresnet50',\n",
    "            \"dataset\": \"600x1000-v6-mass\",\n",
    "            \"epochs\": epochs,\n",
    "            \"batch size\": batch_size,\n",
    "            \"hidden size\": hidden_size, \n",
    "            \"optimizer_name\": 'Adam',\n",
    "            \"dropout_rate\": dropout,\n",
    "            \"weight decay\": 1e-4,       \n",
    "            }\n",
    "        )\n",
    "    \n",
    "    model, train_metrics, val_metrics, sched_steps = train_model(model, criterion, optimizer, [scheduler,scheduler1], num_epochs=epochs)\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=.01)\n",
    "#     scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 1400, T_mult=3, eta_min=1e-12, last_epoch=- 1, verbose=False)\n",
    "#     model, train_metrics, val_metrics, sched_steps = train_model(model, criterion, optimizer, [scheduler,scheduler1], num_epochs=12)\n",
    "\n",
    "    test_metrics = test_model(model, criterion,test_dataloader, threshold = .4)\n",
    "    wandb.finish() \n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "#     PATH='/home/mbadhan/Desktop/mberghouse/pytorch_models/weights/4_16/densenet161_mass_600x1000_run'+str(i)+'_weights'\n",
    "#     torch.save(model.state_dict(), PATH)\n",
    "#     PATH='/home/mbadhan/Desktop/mberghouse/pytorch_models/models/4_16/densenet161_mass_600x1000_run'+str(i)+'_model'\n",
    "#     torch.save(model, PATH)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "--------------------\n",
      "train for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.47404471039772034\n",
      "train F1 is 0.6691729426383972\n",
      "val for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4338589012622833\n",
      "val F1 is 0.6461538672447205\n",
      "Epoch 1/12, learning rate: 1.1011732026620443e-06\n",
      "Train Loss: 0.5989, Train Acc: 0.6823, Train f1: 0.6692, Train Precision: 0.6654, Train Recall: 0.6730, Train AUC: 0.7460\n",
      "Valitadion Loss: 0.5879, Validation Acc: 0.6814, Vall f1: 0.6462, Val Precision: 0.5769, Val Recall: 0.7343, Val AUC: 0.7635\n",
      "train for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.47738537192344666\n",
      "train F1 is 0.6737043857574463\n",
      "val for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4312981367111206\n",
      "val F1 is 0.649350643157959\n",
      "Epoch 2/12, learning rate: 3.349385939948222e-05\n",
      "Train Loss: 0.5998, Train Acc: 0.6931, Train f1: 0.6737, Train Precision: 0.6842, Train Recall: 0.6635, Train AUC: 0.7488\n",
      "Valitadion Loss: 0.5818, Validation Acc: 0.7008, Vall f1: 0.6494, Val Precision: 0.6061, Val Recall: 0.6993, Val AUC: 0.7616\n",
      "train for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.44935616850852966\n",
      "train F1 is 0.6373831629753113\n",
      "val for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.5423796772956848\n",
      "val F1 is 0.6026490330696106\n",
      "Epoch 3/12, learning rate: 3.313570110938815e-05\n",
      "Train Loss: 0.6291, Train Acc: 0.6498, Train f1: 0.6374, Train Precision: 0.6303, Train Recall: 0.6446, Train AUC: 0.6924\n",
      "Valitadion Loss: 0.6395, Validation Acc: 0.6676, Vall f1: 0.6026, Val Precision: 0.5723, Val Recall: 0.6364, Val AUC: 0.7366\n",
      "train for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4820822477340698\n",
      "train F1 is 0.6297376155853271\n",
      "val for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.3708682358264923\n",
      "val F1 is 0.6568915247917175\n",
      "Epoch 4/12, learning rate: 3.2156693574738774e-05\n",
      "Train Loss: 0.6214, Train Acc: 0.6561, Train f1: 0.6297, Train Precision: 0.6480, Train Recall: 0.6125, Train AUC: 0.7117\n",
      "Valitadion Loss: 0.5871, Validation Acc: 0.6759, Vall f1: 0.6569, Val Precision: 0.5657, Val Recall: 0.7832, Val AUC: 0.7618\n",
      "train for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.45835354924201965\n",
      "train F1 is 0.6456400752067566\n",
      "val for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.39282315969467163\n",
      "val F1 is 0.6292834877967834\n",
      "Epoch 5/12, learning rate: 3.059392565550708e-05\n",
      "Train Loss: 0.6285, Train Acc: 0.6552, Train f1: 0.6456, Train Precision: 0.6339, Train Recall: 0.6578, Train AUC: 0.6999\n",
      "Valitadion Loss: 0.5929, Validation Acc: 0.6704, Vall f1: 0.6293, Val Precision: 0.5674, Val Recall: 0.7063, Val AUC: 0.7449\n",
      "train for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4525357484817505\n",
      "train F1 is 0.6752058267593384\n",
      "val for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.32018133997917175\n",
      "val F1 is 0.6727828979492188\n",
      "Epoch 6/12, learning rate: 2.8506601472678112e-05\n",
      "Train Loss: 0.6044, Train Acc: 0.6796, Train f1: 0.6752, Train Precision: 0.6543, Train Recall: 0.6975, Train AUC: 0.7346\n",
      "Valitadion Loss: 0.5738, Validation Acc: 0.7036, Vall f1: 0.6728, Val Precision: 0.5978, Val Recall: 0.7692, Val AUC: 0.7719\n",
      "train for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4429011344909668\n",
      "train F1 is 0.6792114973068237\n",
      "val for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4646683633327484\n",
      "val F1 is 0.641269862651825\n",
      "Epoch 7/12, learning rate: 2.5973797510952155e-05\n",
      "Train Loss: 0.5939, Train Acc: 0.6769, Train f1: 0.6792, Train Precision: 0.6457, Train Recall: 0.7164, Train AUC: 0.7452\n",
      "Valitadion Loss: 0.5871, Validation Acc: 0.6870, Vall f1: 0.6413, Val Precision: 0.5872, Val Recall: 0.7063, Val AUC: 0.7691\n",
      "train for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.47134312987327576\n",
      "train F1 is 0.6780626773834229\n",
      "val for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.49608859419822693\n",
      "val F1 is 0.6689895391464233\n",
      "Epoch 8/12, learning rate: 2.309146687400896e-05\n",
      "Train Loss: 0.5759, Train Acc: 0.6940, Train f1: 0.6781, Train Precision: 0.6813, Train Recall: 0.6749, Train AUC: 0.7637\n",
      "Valitadion Loss: 0.5602, Validation Acc: 0.7368, Vall f1: 0.6690, Val Precision: 0.6667, Val Recall: 0.6713, Val AUC: 0.7942\n",
      "train for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.49146077036857605\n",
      "train F1 is 0.680975615978241\n",
      "val for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.43395087122917175\n",
      "val F1 is 0.6881029009819031\n",
      "Epoch 9/12, learning rate: 1.9968804183550497e-05\n",
      "Train Loss: 0.5689, Train Acc: 0.7049, Train f1: 0.6810, Train Precision: 0.7036, Train Recall: 0.6597, Train AUC: 0.7704\n",
      "Valitadion Loss: 0.5517, Validation Acc: 0.7313, Vall f1: 0.6881, Val Precision: 0.6369, Val Recall: 0.7483, Val AUC: 0.8000\n",
      "train for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.46492141485214233\n",
      "train F1 is 0.7236467003822327\n",
      "val for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4139639735221863\n",
      "val F1 is 0.6600660085678101\n",
      "Epoch 10/12, learning rate: 1.6724108834801293e-05\n",
      "Train Loss: 0.5392, Train Acc: 0.7374, Train f1: 0.7236, Train Precision: 0.7271, Train Recall: 0.7202, Train AUC: 0.8029\n",
      "Valitadion Loss: 0.5455, Validation Acc: 0.7147, Vall f1: 0.6601, Val Precision: 0.6250, Val Recall: 0.6993, Val AUC: 0.7932\n",
      "train for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.47940316796302795\n",
      "train F1 is 0.7153699994087219\n",
      "val for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.3686620593070984\n",
      "val F1 is 0.6537216901779175\n",
      "Epoch 11/12, learning rate: 1.348030332548027e-05\n",
      "Train Loss: 0.5377, Train Acc: 0.7292, Train f1: 0.7154, Train Precision: 0.7181, Train Recall: 0.7127, Train AUC: 0.8012\n",
      "Valitadion Loss: 0.5719, Validation Acc: 0.7036, Vall f1: 0.6537, Val Precision: 0.6084, Val Recall: 0.7063, Val AUC: 0.7707\n",
      "train for epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:03<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4338982105255127\n",
      "train F1 is 0.752136766910553\n",
      "val for epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.3793574273586273\n",
      "val F1 is 0.6687697172164917\n",
      "Epoch 12/12, learning rate: 1.0360276442503538e-05\n",
      "Train Loss: 0.5081, Train Acc: 0.7644, Train f1: 0.7521, Train Precision: 0.7557, Train Recall: 0.7486, Train AUC: 0.8279\n",
      "Valitadion Loss: 0.5885, Validation Acc: 0.7091, Vall f1: 0.6688, Val Precision: 0.6092, Val Recall: 0.7413, Val AUC: 0.7712\n",
      "Training complete in 14m 11s\n",
      "Best val auc: 0.688103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=6e-5, weight_decay=.001)\n",
    "# scheduler=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 1400, T_mult=3, eta_min=1e-12, last_epoch=- 1, verbose=False)\n",
    "\n",
    "model, train_metrics, val_metrics, sched_steps = train_model(model, criterion, optimizer, [scheduler,scheduler1], num_epochs=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.3793574273586273\n",
      "Inference complete in 0m 8s\n",
      "F1 Score = : 0.668770\n",
      "AUC Score = : 0.771155\n",
      "Acc Score = : 0.709141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td></td></tr><tr><td>test_auc</td><td></td></tr><tr><td>test_f1</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>test_precision</td><td></td></tr><tr><td>test_recall</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_auc</td><td></td></tr><tr><td>train_f1</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>train_precision</td><td></td></tr><tr><td>train_recall</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_auc</td><td></td></tr><tr><td>val_f1</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>0.70914</td></tr><tr><td>test_auc</td><td>0.77116</td></tr><tr><td>test_f1</td><td>0.66877</td></tr><tr><td>test_loss</td><td>0.5885</td></tr><tr><td>test_precision</td><td>0.6092</td></tr><tr><td>test_recall</td><td>0.74126</td></tr><tr><td>train_acc</td><td>0.76444</td></tr><tr><td>train_auc</td><td>0.82792</td></tr><tr><td>train_f1</td><td>0.75214</td></tr><tr><td>train_loss</td><td>0.50809</td></tr><tr><td>train_precision</td><td>0.75573</td></tr><tr><td>train_recall</td><td>0.74858</td></tr><tr><td>val_acc</td><td>0.70914</td></tr><tr><td>val_auc</td><td>0.77116</td></tr><tr><td>val_f1</td><td>0.66877</td></tr><tr><td>val_loss</td><td>0.5885</td></tr><tr><td>val_precision</td><td>0.6092</td></tr><tr><td>val_recall</td><td>0.74126</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-cosmos-433</strong> at: <a href='https://wandb.ai/mammogram_project/Model%20Testing/runs/qno89yh4' target=\"_blank\">https://wandb.ai/mammogram_project/Model%20Testing/runs/qno89yh4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230422_225550-qno89yh4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_metrics = test_model(model, criterion,test_dataloader, threshold = .4)\n",
    "wandb.finish() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "--------------------\n",
      "train for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4850434958934784\n",
      "train F1 is 0.748161792755127\n",
      "val for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.5343158841133118\n",
      "val F1 is 0.6190476417541504\n",
      "Epoch 1/26, learning rate: 8.289033742546635e-07\n",
      "Train Loss: 0.5192, Train Acc: 0.7527, Train f1: 0.7482, Train Precision: 0.7360, Train Recall: 0.7607, Train AUC: 0.8276\n",
      "Valitadion Loss: 0.5821, Validation Acc: 0.6898, Vall f1: 0.6190, Val Precision: 0.6026, Val Recall: 0.6364, Val AUC: 0.7632\n",
      "train for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.5007456541061401\n",
      "train F1 is 0.7214353084564209\n",
      "val for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.5044635534286499\n",
      "val F1 is 0.6600660085678101\n",
      "Epoch 2/26, learning rate: 5.98485305119635e-05\n",
      "Train Loss: 0.5327, Train Acc: 0.7338, Train f1: 0.7214, Train Precision: 0.7290, Train Recall: 0.7140, Train AUC: 0.8143\n",
      "Valitadion Loss: 0.5757, Validation Acc: 0.7147, Vall f1: 0.6601, Val Precision: 0.6250, Val Recall: 0.6993, Val AUC: 0.7872\n",
      "train for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.45667606592178345\n",
      "train F1 is 0.7145454287528992\n",
      "val for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.3406563997268677\n",
      "val F1 is 0.6133333444595337\n",
      "Epoch 3/26, learning rate: 5.848769263184594e-05\n",
      "Train Loss: 0.5483, Train Acc: 0.7166, Train f1: 0.7145, Train Precision: 0.6956, Train Recall: 0.7346, Train AUC: 0.7926\n",
      "Valitadion Loss: 0.5929, Validation Acc: 0.6787, Vall f1: 0.6133, Val Precision: 0.5860, Val Recall: 0.6434, Val AUC: 0.7460\n",
      "train for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4732363224029541\n",
      "train F1 is 0.7063196897506714\n",
      "val for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.8456973433494568\n",
      "val F1 is 0.6426229476928711\n",
      "Epoch 4/26, learning rate: 5.5774157508658885e-05\n",
      "Train Loss: 0.5552, Train Acc: 0.7148, Train f1: 0.7063, Train Precision: 0.7024, Train Recall: 0.7103, Train AUC: 0.7866\n",
      "Valitadion Loss: 0.9490, Validation Acc: 0.6981, Vall f1: 0.6426, Val Precision: 0.6049, Val Recall: 0.6853, Val AUC: 0.7490\n",
      "train for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4766172766685486\n",
      "train F1 is 0.7486136555671692\n",
      "val for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.35340216755867004\n",
      "val F1 is 0.6910299062728882\n",
      "Epoch 5/26, learning rate: 5.183677346971223e-05\n",
      "Train Loss: 0.5106, Train Acc: 0.7545, Train f1: 0.7486, Train Precision: 0.7404, Train Recall: 0.7570, Train AUC: 0.8287\n",
      "Valitadion Loss: 0.5716, Validation Acc: 0.7424, Vall f1: 0.6910, Val Precision: 0.6582, Val Recall: 0.7273, Val AUC: 0.7824\n",
      "train for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4876211881637573\n",
      "train F1 is 0.7587511539459229\n",
      "val for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.6851308345794678\n",
      "val F1 is 0.6185566782951355\n",
      "Epoch 6/26, learning rate: 4.686250155452223e-05\n",
      "Train Loss: 0.4761, Train Acc: 0.7699, Train f1: 0.7588, Train Precision: 0.7682, Train Recall: 0.7495, Train AUC: 0.8542\n",
      "Valitadion Loss: 0.6754, Validation Acc: 0.6925, Vall f1: 0.6186, Val Precision: 0.6081, Val Recall: 0.6294, Val AUC: 0.7505\n",
      "train for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.478832870721817\n",
      "train F1 is 0.7786116600036621\n",
      "val for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.2581902742385864\n",
      "val F1 is 0.6686747074127197\n",
      "Epoch 7/26, learning rate: 4.10875379377321e-05\n",
      "Train Loss: 0.4611, Train Acc: 0.7870, Train f1: 0.7786, Train Precision: 0.7815, Train Recall: 0.7757, Train AUC: 0.8654\n",
      "Valitadion Loss: 0.6059, Validation Acc: 0.6953, Vall f1: 0.6687, Val Precision: 0.5873, Val Recall: 0.7762, Val AUC: 0.7689\n",
      "train for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.47477221488952637\n",
      "train F1 is 0.7946767807006836\n",
      "val for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.46025824546813965\n",
      "val F1 is 0.6689655184745789\n",
      "Epoch 8/26, learning rate: 3.4786098492186285e-05\n",
      "Train Loss: 0.4375, Train Acc: 0.8051, Train f1: 0.7947, Train Precision: 0.8085, Train Recall: 0.7813, Train AUC: 0.8806\n",
      "Valitadion Loss: 0.5601, Validation Acc: 0.7341, Vall f1: 0.6690, Val Precision: 0.6599, Val Recall: 0.6783, Val AUC: 0.7913\n",
      "train for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.46665090322494507\n",
      "train F1 is 0.8317757248878479\n",
      "val for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.35380125045776367\n",
      "val F1 is 0.668749988079071\n",
      "Epoch 9/26, learning rate: 2.8257398041111235e-05\n",
      "Train Loss: 0.3970, Train Acc: 0.8375, Train f1: 0.8318, Train Precision: 0.8318, Train Recall: 0.8318, Train AUC: 0.9033\n",
      "Valitadion Loss: 0.6145, Validation Acc: 0.7064, Vall f1: 0.6687, Val Precision: 0.6045, Val Recall: 0.7483, Val AUC: 0.7947\n",
      "train for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.484438955783844\n",
      "train F1 is 0.8427907228469849\n",
      "val for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.6500016450881958\n",
      "val F1 is 0.6571428775787354\n",
      "Epoch 10/26, learning rate: 2.1811442570956206e-05\n",
      "Train Loss: 0.3771, Train Acc: 0.8475, Train f1: 0.8428, Train Precision: 0.8389, Train Recall: 0.8467, Train AUC: 0.9138\n",
      "Valitadion Loss: 0.6554, Validation Acc: 0.7341, Vall f1: 0.6571, Val Precision: 0.6715, Val Recall: 0.6434, Val AUC: 0.7853\n",
      "train for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 139/139 [01:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.4719512462615967\n",
      "train F1 is 0.8444863557815552\n",
      "val for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 46/46 [00:07<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.30895018577575684\n",
      "val F1 is 0.6527777910232544\n",
      "Epoch 11/26, learning rate: 1.575430904131036e-05\n",
      "Train Loss: 0.3435, Train Acc: 0.8511, Train f1: 0.8445, Train Precision: 0.8517, Train Recall: 0.8374, Train AUC: 0.9302\n",
      "Valitadion Loss: 0.7001, Validation Acc: 0.7230, Vall f1: 0.6528, Val Precision: 0.6483, Val Recall: 0.6573, Val AUC: 0.7747\n",
      "train for epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|                                   | 22/139 [00:12<01:06,  1.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1905019/2997613642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msched_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1905019/2692905188.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;31m#print(all_outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/timm/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# both classifier and block drop?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/timm/models/densenet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/timm/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/timm/models/densenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mbottleneck_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottleneck_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottleneck_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/timm/models/layers/norm_act.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[0;32m--> 100\u001b[0;31m         x = F.batch_norm(\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2450\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2451\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, train_metrics, val_metrics, sched_steps = train_model(model, criterion, optimizer, [scheduler,scheduler1], num_epochs=26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:02<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New threshold is 0.3804146647453308\n",
      "Inference complete in 0m 2s\n",
      "F1 Score = : 0.661157\n",
      "AUC Score = : 0.756051\n",
      "Acc Score = : 0.713287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df7bcc2d5ee4538a6747467275017da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td></td></tr><tr><td>test_auc</td><td></td></tr><tr><td>test_f1</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>test_precision</td><td></td></tr><tr><td>test_recall</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_auc</td><td></td></tr><tr><td>train_f1</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>train_precision</td><td></td></tr><tr><td>train_recall</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_auc</td><td></td></tr><tr><td>val_f1</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>val_precision</td><td></td></tr><tr><td>val_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_acc</td><td>0.71329</td></tr><tr><td>test_auc</td><td>0.75605</td></tr><tr><td>test_f1</td><td>0.66116</td></tr><tr><td>test_loss</td><td>0.69239</td></tr><tr><td>test_precision</td><td>0.65041</td></tr><tr><td>test_recall</td><td>0.67227</td></tr><tr><td>train_acc</td><td>0.85611</td></tr><tr><td>train_auc</td><td>0.93031</td></tr><tr><td>train_f1</td><td>0.83659</td></tr><tr><td>train_loss</td><td>0.34682</td></tr><tr><td>train_precision</td><td>0.7782</td></tr><tr><td>train_recall</td><td>0.90444</td></tr><tr><td>val_acc</td><td>0.70629</td></tr><tr><td>val_auc</td><td>0.75192</td></tr><tr><td>val_f1</td><td>0.664</td></tr><tr><td>val_loss</td><td>0.68222</td></tr><tr><td>val_precision</td><td>0.63359</td></tr><tr><td>val_recall</td><td>0.69748</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-valley-6</strong> at: <a href='https://wandb.ai/mammogram_project/FINAL_Calc_ssl_resnet50_baseline_test/runs/ci2an98y' target=\"_blank\">https://wandb.ai/mammogram_project/FINAL_Calc_ssl_resnet50_baseline_test/runs/ci2an98y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230409_184158-ci2an98y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_metrics = test_model(model, criterion,test_dataloader, threshold = .4)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/home/mbadhan/Desktop/mberghouse/pytorch_models/weights/final/SSLresnet50_calc_RMS_run2_weights'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "PATH='/home/mbadhan/Desktop/mberghouse/pytorch_models/models/final/SSLresnet50_calc_RMS_run2_model'\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#efficientnetv2_s\n",
    "#ecaresnext50t_32x4d\n",
    "#ecaresnet50d\n",
    "#ecaresnet50t\n",
    "#ecaresnet101d\n",
    "#efficientnet_b3_gn\n",
    "#efficientnet_b3\n",
    "# 'halo2botnet50ts_256',\n",
    "# , 'halonet26t',\n",
    "# , 'halonet50ts',\n",
    "# , 'halonet_h1',\n",
    "# , 'haloregnetz_b',\n",
    "#legacy_seresnet50\n",
    "#mobilenetv3_large_075\n",
    "#mobilenetv3_small_100\n",
    "#seresnet50\n",
    "#seresnext50_32x4d\n",
    "#skresnet50\n",
    "#ssl_resnet50\n",
    "#swsl_resnet50\n",
    "#visformer_small\n",
    "#tv_resnet50\n",
    "#resnest50d  #split Attention\n",
    "#regnetx_040\n",
    "#twins_pcpvt_base\n",
    "#twins_svt_base\n",
    "\n",
    "### Crossvit Models ###\n",
    "#  'crossvit_9_240',\n",
    "# , 'crossvit_9_dagger_240',\n",
    "# , 'crossvit_15_240',\n",
    "# , 'crossvit_15_dagger_240',\n",
    "# , 'crossvit_15_dagger_408',\n",
    "# , 'crossvit_18_240',\n",
    "# , 'crossvit_18_dagger_240',\n",
    "# , 'crossvit_18_dagger_408',\n",
    "# , 'crossvit_base_240',\n",
    "# , 'crossvit_small_240',\n",
    "# , 'crossvit_tiny_240',\n",
    "\n",
    "### Swin Models ###\n",
    "# , 'swin_base_patch4_window7_224',\n",
    "# , 'swin_base_patch4_window7_224_in22k',\n",
    "# , 'swin_base_patch4_window12_384',\n",
    "# , 'swin_base_patch4_window12_384_in22k',\n",
    "# , 'swin_large_patch4_window7_224',\n",
    "# , 'swin_large_patch4_window7_224_in22k',\n",
    "# , 'swin_large_patch4_window12_384',\n",
    "# , 'swin_large_patch4_window12_384_in22k',\n",
    "# , 'swin_s3_base_224',\n",
    "# , 'swin_s3_small_224',\n",
    "# , 'swin_s3_tiny_224',\n",
    "# , 'swin_small_patch4_window7_224',\n",
    "# , 'swin_tiny_patch4_window7_224',\n",
    "# , 'swinv2_base_window8_256',\n",
    "# , 'swinv2_base_window12_192_22k',\n",
    "# , 'swinv2_base_window12to16_192to256_22kft1k',\n",
    "# , 'swinv2_base_window12to24_192to384_22kft1k',\n",
    "# , 'swinv2_base_window16_256',\n",
    "# , 'swinv2_cr_base_224',\n",
    "# , 'swinv2_cr_base_384',\n",
    "# , 'swinv2_cr_base_ns_224',\n",
    "# , 'swinv2_cr_giant_224',\n",
    "# , 'swinv2_cr_giant_384',\n",
    "# , 'swinv2_cr_huge_224',\n",
    "# , 'swinv2_cr_huge_384',\n",
    "# , 'swinv2_cr_large_224',\n",
    "# , 'swinv2_cr_large_384',\n",
    "# , 'swinv2_cr_small_224',\n",
    "# , 'swinv2_cr_small_384',\n",
    "# , 'swinv2_cr_small_ns_224',\n",
    "# , 'swinv2_cr_tiny_224',\n",
    "# , 'swinv2_cr_tiny_384',\n",
    "# , 'swinv2_cr_tiny_ns_224',\n",
    "# , 'swinv2_large_window12_192_22k',\n",
    "# , 'swinv2_large_window12to16_192to256_22kft1k',\n",
    "# , 'swinv2_large_window12to24_192to384_22kft1k',\n",
    "# , 'swinv2_small_window8_256',\n",
    "# , 'swinv2_small_window16_256',\n",
    "# , 'swinv2_tiny_window8_256',\n",
    "# , 'swinv2_tiny_window16_256',\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
